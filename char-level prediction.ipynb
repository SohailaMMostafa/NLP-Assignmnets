{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wikipedia\n\nimport wikipedia","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-17T22:08:33.352276Z","iopub.execute_input":"2024-05-17T22:08:33.352662Z","iopub.status.idle":"2024-05-17T22:08:49.707082Z","shell.execute_reply.started":"2024-05-17T22:08:33.352630Z","shell.execute_reply":"2024-05-17T22:08:49.705990Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (4.12.2)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wikipedia) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->wikipedia) (2.5)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=d9bb71a3795ccff4ea91c47484e4100b5e6eb872e11a7d27d53c1a970ecec70c\n  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding,SimpleRNN\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re\nimport random\nimport gensim\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport seaborn as sns\n\nnltk.download('wordnet', \"/kaggle/working/nltk_data/\")\nnltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\nnltk.data.path.append(\"/kaggle/working/nltk_data/\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T23:24:40.010930Z","iopub.execute_input":"2024-05-17T23:24:40.011301Z","iopub.status.idle":"2024-05-17T23:25:11.432057Z","shell.execute_reply.started":"2024-05-17T23:24:40.011269Z","shell.execute_reply":"2024-05-17T23:25:11.430987Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data]   Package omw-1.4 is already up-to-date!\nArchive:  /kaggle/working/nltk_data/corpora/wordnet.zip\nreplace /kaggle/working/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\nArchive:  /kaggle/working/nltk_data/corpora/omw-1.4.zip\nreplace /kaggle/working/nltk_data/corpora/omw-1.4/fin/LICENSE? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fetch entire article\nfull_article1 = wikipedia.page(\"Artificial Intelligence\").content","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:24.705061Z","iopub.execute_input":"2024-05-17T22:09:24.706049Z","iopub.status.idle":"2024-05-17T22:09:25.589524Z","shell.execute_reply.started":"2024-05-17T22:09:24.706018Z","shell.execute_reply":"2024-05-17T22:09:25.588443Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Fetch entire article\nfull_article2 = wikipedia.page(\"neural network\").content","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:25.591960Z","iopub.execute_input":"2024-05-17T22:09:25.592268Z","iopub.status.idle":"2024-05-17T22:09:26.393893Z","shell.execute_reply.started":"2024-05-17T22:09:25.592243Z","shell.execute_reply":"2024-05-17T22:09:26.392901Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Fetch entire article\nfull_article3 = wikipedia.page(\"Deep learning\").content","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:26.395546Z","iopub.execute_input":"2024-05-17T22:09:26.395895Z","iopub.status.idle":"2024-05-17T22:09:27.426885Z","shell.execute_reply.started":"2024-05-17T22:09:26.395865Z","shell.execute_reply":"2024-05-17T22:09:27.425958Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"documents = [[full_article1], [full_article2], [full_article3]]\ndocuments","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:27.429089Z","iopub.execute_input":"2024-05-17T22:09:27.429805Z","iopub.status.idle":"2024-05-17T22:09:27.444677Z","shell.execute_reply.started":"2024-05-17T22:09:27.429770Z","shell.execute_reply":"2024-05-17T22:09:27.443690Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"[['Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and uses learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.\\nAI technology is widely used throughout industry, government, and science. Some high-profile applications include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it\\'s not labeled AI anymore.\"\\nAlan Turing was the first person to conduct substantial research in the field that he called machine intelligence. Artificial intelligence was founded as an academic discipline in 1956. The field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning surpassed all previous AI techniques, and after 2017 with the transformer architecture. This led to the AI boom of the early 2020s, with companies, universities, and laboratories overwhelmingly based in the United States pioneering significant advances in artificial intelligence.\\nThe growing use of artificial intelligence in the 21st century is influencing a societal and economic shift towards increased automation, data-driven decision-making, and the integration of AI systems into various economic sectors and areas of life, impacting job markets, healthcare, government, industry, and education. This raises questions about the long-term effects, ethical implications, and risks of AI, prompting discussions about regulatory policies to ensure the safety and benefits of the technology. \\nThe various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field\\'s long-term goals.\\nTo reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\\n\\n\\n=== Reasoning and problem-solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\\n\\n\\n=== Planning and decision-making ===\\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\\nIn some problems, the agent\\'s preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\\n\\n\\n=== Learning ===\\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\\n\\n\\n=== Natural language processing ===\\nNatural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\\nEarly work, based on Noam Chomsky\\'s generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023 these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\\n\\n\\n=== Perception ===\\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\\nThe field includes speech recognition, image classification, facial recognition, object recognition, and robotic perception.\\n\\n\\n=== Social intelligence ===\\n\\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.\\n\\n\\n=== General intelligence ===\\nA machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\\n\\n\\n== Techniques ==\\nAI research uses a wide variety of techniques to accomplish the goals above.\\n\\n\\n=== Search and optimization ===\\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\\n\\n\\n==== State space search ====\\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.\\n\\n\\n==== Local search ====\\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.\\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\\n\\n\\n=== Logic ===\\nFormal logic is used for reasoning and knowledge representation.\\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.\\nOther specialized versions of logic have been developed to describe many complex domains.\\n\\n\\n=== Probabilistic methods for uncertain reasoning ===\\n\\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\\n\\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\\n\\n\\n=== Classifiers and statistical learning methods ===\\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\\nNeural networks are also used as classifiers.\\n\\n\\n=== Artificial neural networks ===\\n\\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.\\nNeural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\\nIn feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.\\nPerceptrons\\nuse only a single layer of neurons, deep learning uses multiple layers.\\nConvolutional neural networks strengthen the connection between neurons that are \"close\" to each other—this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.\\n\\n\\n=== Deep learning ===\\n\\nDeep learning\\nuses several layers of neurons between the network\\'s inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023.\\nThe sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)\\nbut because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\\n\\n\\n=== GPT ===\\nGenerative pre-trained transformers (GPT) are large language models that are based on the semantic relationships between words in sentences (natural language processing). Text-based GPT models are pre-trained on a large corpus of text which can be from the internet. The pre-training consists in predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pre-training, GPT models accumulate knowledge about the world, and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are still prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow you to ask a question or request a task in simple text.\\nCurrent models and services include: Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\\n\\n\\n=== Specialized hardware and software ===\\n\\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models\\' training. Historically, specialized languages, such as Lisp, Prolog, Python and others, had been used.\\n\\n\\n== Applications ==\\nAI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple\\'s Face ID or Microsoft\\'s DeepFace and Google\\'s FaceNet) and image labeling (used by Facebook, Apple\\'s iPhoto and TikTok).\\n\\n\\n=== Health and medicine ===\\n\\nThe application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.\\nFor medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson\\'s disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson\\'s disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\\n\\n\\n=== Games ===\\n\\nGame playing programs have been used since the 1950s to demonstrate and test AI\\'s most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM\\'s question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then in 2017 it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind\\'s AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world\\'s best Gran Turismo drivers using deep reinforcement learning.\\n\\n\\n=== Military ===\\n\\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.\\nIn November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.\\n\\n\\n=== Generative AI ===\\n\\nIn the early 2020s, generative AI gained widespread prominence. In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.\\n\\n\\n=== Industry-specific tasks ===\\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\\n\\n\\n== Ethics ==\\n\\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\\n\\n\\n=== Risks and harm ===\\n\\n\\n==== Privacy and copyright ====\\n\\nMachine-learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\\nTechnology companies collect a wide range of data from their users, including online activity, geolocation data, video and audio.\\nFor example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\\nAI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of \\'what they know\\' to the question of \\'what they\\'re doing with it\\'.\"\\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\\n\\n\\n==== Misinformation ====\\n\\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem.\\nIn 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.\\n\\n\\n==== Algorithmic bias and fairness ====\\n\\nMachine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists.\\nBias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.\\nFairness in machine learning is the study of how to prevent the harm caused by algorithmic bias. It has become serious area of academic study within AI. Researchers have discovered it is not always possible to define \"fairness\" in a way that satisfies all stakeholders.\\nOn June 28, 2015, Google Photos\\'s new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist.\\nIn 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".\\nMoritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn\\'t work.\"\\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is necessarily descriptive and not proscriptive.\\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\\n\\n\\n==== Lack of transparency ====\\n\\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\\nPeople who have been harmed by an algorithm\\'s decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union\\'s General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try and solve these problems. \\nThere are several possible solutions to the transparency problem. SHAP tried to solve the transparency problems by visualising the contribution of each feature to the output. LIME can locally approximate a model with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network have learned and produce output that can suggest what the network is learning.\\n\\n\\n==== Bad actors and weaponized AI ====\\n\\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations\\' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\\nThere many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\\n\\n\\n==== Reliance on industry giants ====\\nTraining AI systems requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller startups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.\\n\\n\\n==== Technological unemployment ====\\n\\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we\\'re in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\\n\\n\\n==== Existential risk ====\\n\\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\\nFirst, AI does not require human-like \"sentience\" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can\\'t fetch the coffee if you\\'re dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity\\'s morality and values so that it is \"fundamentally on our side\".\\nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\\nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk have expressed concern about existential risk from AI.\\nAI pioneers including Fei-Fei Li, Geoffrey Hinton, Yoshua Bengio, Cynthia Breazeal, Rana el Kaliouby, Demis Hassabis, Joy Buolamwini, and Sam Altman have expressed concerns about the risks of AI. In 2023, many leading AI experts issued the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\\nOther researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it\\'s a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers\\' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\\n\\n\\n=== Ethical machines and alignment ===\\n\\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\\nThe field of machine ethics is also called computational morality,\\nand was founded at an AAAI symposium in 2005.\\nOther approaches include Wendell Wallach\\'s \"artificial moral agents\" and Stuart J. Russell\\'s three principles for developing provably beneficial machines.\\n\\n\\n=== Open source ===\\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism), and that once released on the Internet, they can\\'t be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\\n\\n\\n=== Frameworks ===\\nArtificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:\\n\\nRESPECT the dignity of individual people\\nCONNECT with other people sincerely, openly and inclusively\\nCARE for the wellbeing of everyone\\nPROTECT social values, justice and the public interest\\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE\\'s Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.\\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\\nThe AI Safety Institute in the UK has released a testing toolset called ‘Inspect’ for AI safety evaluations available under a MIT open-source licence which is freely available on Github and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\\n\\n\\n=== Regulation ===\\n\\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.\\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.\\n\\n\\n== History ==\\n\\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing\\'s theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". \\nThey developed several areas of research that would become part of AI,\\nsuch as McCullouch and Pitts design for \"artificial neurons\" in 1943, and Turing\\'s influential 1950 paper \\'Computing Machinery and Intelligence\\', which introduced the Turing test and showed that \"machine intelligence\" was plausible.\\nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. Universities in the latter 1950s and early 1960s.\\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky\\'s and Papert\\'s book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan\\'s fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\\nUp to this point, most of AI\\'s funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".\\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\\nFor many specific tasks, other methods were abandoned.\\nDeep learning\\'s success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning\\'s success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\\nIn the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".\\nAbout 800,000 \"AI\"-related U.S. job openings existed in 2022.\\n\\n\\n== Philosophy ==\\n\\n\\n=== Defining artificial intelligence ===\\n\\nAlan Turing wrote in 1950 \"I propose to consider the question \\'can machines think\\'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks\"\\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making \\'machines that fly so exactly like pigeons that they can fool other pigeons.\\'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\\n\\n\\n=== Evaluating approaches to AI ===\\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\\n\\n\\n==== Symbolic AI and its limits ====\\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec\\'s paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\\n\\n\\n==== Neat vs. scruffy ====\\n\\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\\n\\n\\n==== Soft vs. hard computing ====\\n\\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\\n\\n\\n==== Narrow vs. general AI ====\\n\\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field\\'s long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\\n\\n\\n=== Machine consciousness, sentience, and mind ===\\n\\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\\n\\n\\n==== Consciousness ====\\n\\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett\\'s consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\\n\\n\\n==== Computationalism and functionalism ====\\n\\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\\n\\n\\n==== AI welfare and rights ====\\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.\\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\\n\\n\\n== Future ==\\n\\n\\n=== Superintelligence and the singularity ===\\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.\\nIf research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\\n\\n\\n=== Transhumanism ===\\nRobot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.\\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler\\'s \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\\n\\n\\n== In fiction ==\\n\\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\\nA common trope in these works began with Mary Shelley\\'s Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\\nIsaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov\\'s laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek\\'s R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\\n\\n\\n== See also ==\\nArtificial intelligence detection software – Software to detect AI-generated contentPages displaying short descriptions of redirect targets\\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\\nBusiness process automation – Technology-enabled automation of complex business processes\\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\\nDigital immortality – Hypothetical concept of storing a personality in digital form\\nEmergent algorithm – Algorithm exhibiting emergent behavior\\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\\nIntelligence amplification – Use of information technology to augment human intelligence\\nMind uploading – Hypothetical process of digitally emulating a brain\\nRobotic process automation – Form of business process automation technology\\nWeak artificial intelligence – Form of artificial intelligence\\nWetware computer – Computer composed of organic material\\n\\n\\n== Explanatory notes ==\\n\\n\\n== References ==\\n\\n\\n=== AI textbooks ===\\nThe two most widely used textbooks in 2023. (See the Open Syllabus).\\n\\nRussell, Stuart J.; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0134610993. LCCN 20190474.\\nRich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0070087705.\\nThese were the four of the most widely used AI textbooks in 2008:\\n\\n\\n=== History of AI ===\\n\\n\\n=== Other sources ===\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\\nThomason, Richmond. \"Logic and Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005).\\nTheranostics and AI – The Next Advance in Cancer Precision Medicine'],\n ['A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.\\n\\nIn neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses.\\nIn machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.\\n\\n\\n== In biology ==\\n\\nIn the context of biology, a neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.\\nEach neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.\\nPopulations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.\\nSignals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.\\n\\n\\n== In machine learning ==\\n\\nIn the context of machine learning, a neural network is an artificial mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software.\\nNeurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer).\\nThe \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.\\nNeural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.\\n\\n\\n== History ==\\n\\nThe theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.\\nArtificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957,\\nartificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.\\n\\n\\n== See also ==\\nEmergence\\nBiological cybernetics\\nBiologically-inspired computing\\n\\n\\n== References =='],\n ['Deep learning is the subset of machine learning methods based on neural networks with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.\\nDeep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\\nEarly forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, in particular the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low quality models for that purpose.\\n\\n\\n== Overview ==\\nMost modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\\nFundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\\nImportantly, a deep learning process can learn which features to optimally place in which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate upon. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.\\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.\\n\\n\\n== Interpretations ==\\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\\nThe classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima\\'s rectified linear unit.\\nThe universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\\n\\n\\n== History ==\\nThere were two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don\\'t. In the 1920s, Wilhelm Lenz and Ernst Ising created and analyzed the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun\\'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982.\\nCharles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today, referring to Rosenblatt\\'s 1962 book which introduced multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. It also introduced variants, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).:\\u200asection 16\\u200a In addition, term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.\\nThe first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.\\nThe first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\\'ichi Amari. In computer experiments conducted by Amari\\'s student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples, but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.  Instead, subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\\nIn 1970, Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. \\nThe terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985, David E. Rumelhart et al. published an experimental analysis of the technique.\\nDeep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision.\\nThe term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.\\nIn 1988, Wei Zhang et al. applied the backpropagation algorithm \\nto a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. \\nIn 1989, Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. Subsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a chunker solved a deep learning task whose depth exceeded 1000.\\nIn 1992, Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern.\\nThe modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper \"Attention Is All You Need\". \\nIt combines this with a softmax operator and a projection matrix.\\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.\\nIn 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network\\'s gain is the other network\\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\\nSepp Hochreiter\\'s diploma thesis (1991) was called \"one of the most important documents in the history of machine learning\" by his supervisor Schmidhuber. It not only tested the neural history compressor, but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997. LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.\\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.\\nIn 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.\\nIn 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.\\nSince 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.\\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural networks\\' computational cost and a lack of understanding of how the brain wires its biological networks.\\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\\'s NSA and DARPA, SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.\\nSpeech recognition was taken over by LSTM. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google\\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.\\nThe impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.\\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.\\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.\\nDeep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision.\\nAdvances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the \"big bang\" of deep learning, \"as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)\". That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.\\n\\n\\n=== Deep learning revolution ===\\n\\nIn the late 2000s, deep learning started to outperform other methods in machine learning competitions.\\nIn 2009, a long short-term memory trained by connectionist temporal classification (Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, 2006) was the first RNN to win pattern recognition contests, winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.\\nSignificant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011, the DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. Also in 2011, DanNet won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.  In September 2012, DanNet also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012, the similar AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. \\nThe VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and\\nwon the ImageNet 2014 competition, following a similar trend in large-scale speech recognition.\\nImage classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.\\nIn 2012, a team led by George E. Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Sepp Hochreiter\\'s group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the \"Tox21 Data Challenge\" of NIH, FDA and NCATS.\\nIn 2016, Roger Parloff mentioned a \"deep learning revolution\" that has transformed the AI industry.\\nIn March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.\\n\\n\\n== Neural networks ==\\n\\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\").\\n\\n\\n=== Deep neural networks ===\\nA deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.\\nFor example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.\\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.\\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\\nRecurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.\\nConvolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\\n\\n\\n==== Challenges ====\\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko\\'s unit pruning or weight decay (\\n  \\n    \\n      \\n        \\n          ℓ\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\ell _{2}}\\n  \\n-regularization) or sparsity (\\n  \\n    \\n      \\n        \\n          ℓ\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\ell _{1}}\\n  \\n-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.\\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.\\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn\\'t require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.\\n\\n\\n== Hardware ==\\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.\\nSpecial electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).\\nAtomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).\\nIn 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.\\n\\n\\n== Applications ==\\n\\n\\n=== Automatic speech recognition ===\\n\\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.\\nThe initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\\n\\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:\\n\\nScale-up/out and accelerated DNN training and decoding\\nSequence discriminative training\\nFeature processing by deep models with solid understanding of the underlying mechanisms\\nAdaptation of DNNs and related deep models\\nMulti-task and transfer learning by DNNs and related deep models\\nCNNs and how to design them to best exploit domain knowledge of speech\\nRNN and its rich LSTM variants\\nOther types of deep models including tensor-based models and integrated deep generative/discriminative models.\\nAll major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\\n\\n\\n=== Image recognition ===\\n\\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.\\nDeep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.\\nDeep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\\n\\n\\n=== Visual art processing ===\\n\\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\\n\\nidentifying the style period of a given painting\\nNeural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video\\ngenerating striking imagery based on random visual input fields.\\n\\n\\n=== Natural language processing ===\\n\\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.\\nOther key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.\\nRecent developments generalize word embedding to sentence embedding.\\nGoogle Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.\\n\\n\\n=== Drug discovery and toxicology ===\\n\\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.\\nAtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.\\nIn 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.\\n\\n\\n=== Customer relationship management ===\\n\\nDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.\\n\\n\\n=== Recommendation systems ===\\n\\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\\n\\n\\n=== Bioinformatics ===\\n\\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.\\nIn medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.\\nDeep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.\\n\\n\\n=== Deep Neural Network Estimations ===\\nDeep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels\\' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.\\n\\n\\n=== Medical image analysis ===\\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.\\n\\n\\n=== Mobile advertising ===\\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\\n\\n\\n=== Image restoration ===\\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\\n\\n\\n=== Financial fraud detection ===\\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.\\n\\n\\n=== Materials science ===\\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system\\'s predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.\\n\\n\\n=== Military ===\\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.\\n\\n\\n=== Partial differential equations ===\\nPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.\\n\\n\\n=== Image reconstruction ===\\nImage reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.\\n\\n\\n=== Epigenetic clock ===\\n\\nAn epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\\n\\n\\n== Relation to human cognitive and brain development ==\\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant\\'s brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".\\nA variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.\\nAlthough a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.\\n\\n\\n== Commercial activity ==\\nFacebook\\'s AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.\\nGoogle\\'s DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.\\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.\\nAs of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".\\n\\n\\n== Criticism and comment ==\\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\\n\\n\\n=== Theory ===\\n\\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.\\nOthers point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:\\n\\nRealistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\\n\\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian\\'s website.\\n\\n\\n=== Errors ===\\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).\\n\\n\\n=== Cyber threat ===\\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".\\nIn 2016 researchers used one ANN to doctor images in trial and error fashion, identify another\\'s focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.\\nAnother group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.\\nANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.\\nIn 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".\\nIn \"data poisoning\", false data is continually smuggled into a machine learning system\\'s training set to prevent it from achieving mastery.\\n\\n\\n=== Data collection ethics ===\\n\\nMost Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.\\nMühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook\\'s face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose, Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether or not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate \"a constant stream of verification data\" to further train the network in real-time. As Mühlhoff argues, the involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as \"human-aided artificial intelligence\".\\n\\n\\n== See also ==\\nApplications of artificial intelligence\\nComparison of deep learning software\\nCompressed sensing\\nDifferentiable programming\\nEcho state network\\nList of artificial intelligence projects\\nLiquid state machine\\nList of datasets for machine-learning research\\nReservoir computing\\nScale space and deep learning\\nSparse coding\\nStochastic parrot\\nTopological deep learning\\n\\n\\n== References ==\\n\\n\\n== Further reading ==']]"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess(documents):\n    processed_documents = []\n    for document in documents:\n        processed_doc = str(document)\n        processed_doc = re.sub(r'[^a-zA-Z\\s]', '', processed_doc)\n        processed_doc = processed_doc.lower()\n        \n        sentaces_minus_sw = []\n        stop_words = stopwords.words('english')\n        processed_doc = processed_doc.split()\n        processed_doc = [sentaces_minus_sw.append(word) for word in processed_doc if word not in stop_words]\n        processed_doc = ' '.join(sentaces_minus_sw)\n        \n        lemmatizer = WordNetLemmatizer()\n        processed_doc = processed_doc.split()\n        processed_doc = [lemmatizer.lemmatize(w) for w in processed_doc]\n        processed_doc = ' '.join(processed_doc)\n\n        processed_documents.append(processed_doc)\n\n    return processed_documents","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:27.445968Z","iopub.execute_input":"2024-05-17T22:09:27.446811Z","iopub.status.idle":"2024-05-17T22:09:27.455841Z","shell.execute_reply.started":"2024-05-17T22:09:27.446779Z","shell.execute_reply":"2024-05-17T22:09:27.454939Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"processed_docs = preprocess(documents)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:27.456842Z","iopub.execute_input":"2024-05-17T22:09:27.457161Z","iopub.status.idle":"2024-05-17T22:09:29.592087Z","shell.execute_reply.started":"2024-05-17T22:09:27.457132Z","shell.execute_reply":"2024-05-17T22:09:29.591118Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"processed_docs","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:29.593423Z","iopub.execute_input":"2024-05-17T22:09:29.594226Z","iopub.status.idle":"2024-05-17T22:09:29.602894Z","shell.execute_reply.started":"2024-05-17T22:09:29.594193Z","shell.execute_reply":"2024-05-17T22:09:29.601911Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['artificial intelligence ai broadest sense intelligence exhibited machine particularly computer system field research computer science develops study method software enable machine perceive environment us learning intelligence take action maximize chance achieving defined goal machine may called aisnai technology widely used throughout industry government science highprofile application include advanced web search engine eg google search recommendation system used youtube amazon netflix interacting via human speech eg google assistant siri alexa autonomous vehicle eg waymo generative creative tool eg chatgpt ai art superhuman play analysis strategy game eg chess go however many ai application perceived ai lot cutting edge ai filtered general application often without called ai something becomes useful enough common enough labeled ai anymorenalan turing first person conduct substantial research field called machine intelligence artificial intelligence founded academic discipline field went multiple cycle optimism followed period disappointment loss funding known ai winter funding interest vastly increased deep learning surpassed previous ai technique transformer architecture led ai boom early company university laboratory overwhelmingly based united state pioneering significant advance artificial intelligencenthe growing use artificial intelligence st century influencing societal economic shift towards increased automation datadriven decisionmaking integration ai system various economic sector area life impacting job market healthcare government industry education raise question longterm effect ethical implication risk ai prompting discussion regulatory policy ensure safety benefit technology nthe various subfields ai research centered around particular goal use particular tool traditional goal ai research include reasoning knowledge representation planning learning natural language processing perception support robotics general intelligencethe ability complete task performable human least equal levelis among field longterm goalsnto reach goal ai researcher adapted integrated wide range technique including search mathematical optimization formal logic artificial neural network method based statistic operation research economics ai also draw upon psychology linguistics philosophy neuroscience fieldsnnn goal nthe general problem simulating creating intelligence broken subproblems consist particular trait capability researcher expect intelligent system display trait described received attention cover scope ai researchnnn reasoning problemsolving nearly researcher developed algorithm imitated stepbystep reasoning human use solve puzzle make logical deduction late method developed dealing uncertain incomplete information employing concept probability economicsnmany algorithm insufficient solving large reasoning problem experience combinatorial explosion become exponentially slower problem grow even human rarely use stepbystep deduction early ai research could model solve problem using fast intuitive judgment accurate efficient reasoning unsolved problemnnn knowledge representation nnknowledge representation knowledge engineering allow ai program answer question intelligently make deduction realworld fact formal knowledge representation used contentbased indexing retrieval scene interpretation clinical decision support knowledge discovery mining interesting actionable inference large database areasna knowledge base body knowledge represented form used program ontology set object relation concept property used particular domain knowledge knowledge base need represent thing object property category relation object situation event state time cause effect knowledge knowledge know people know default reasoning thing human assume true told differently remain true even fact changing many aspect domain knowledgenamong difficult problem knowledge representation breadth commonsense knowledge set atomic fact average person know enormous subsymbolic form commonsense knowledge much people know represented fact statement could express verbally also difficulty knowledge acquisition problem obtaining knowledge ai applicationsnnn planning decisionmaking nan agent anything perceives take action world rational agent goal preference take action make happen automated planning agent specific goal automated decisionmaking agent preferencesthere situation would prefer situation trying avoid decisionmaking agent assigns number situation called utility measure much agent prefers possible action calculate expected utility utility possible outcome action weighted probability outcome occur choose action maximum expected utilitynin classical planning agent know exactly effect action realworld problem however agent may certain situation unknown unobservable may know certain happen possible action deterministic must choose action making probabilistic guess reassess situation see action workednin problem agent preference may uncertain especially agent human involved learned eg inverse reinforcement learning agent seek information improve preference information value theory used weigh value exploratory experimental action space possible future action situation typically intractably large agent must take action evaluate situation uncertain outcome bena markov decision process transition model describes probability particular action change state particular way reward function supply utility state cost action policy associate decision possible state policy could calculated eg iteration heuristic learnedngame theory describes rational behavior multiple interacting agent used ai program make decision involve agentsnnn learning nmachine learning study program improve performance given task automatically part ai beginningnthere several kind machine learning unsupervised learning analyzes stream data find pattern make prediction without guidance supervised learning requires human label input data first come two main variety classification program must learn predict category input belongs regression program must deduce numeric function based numeric inputnin reinforcement learning agent rewarded good response punished bad one agent learns choose response classified good transfer learning knowledge gained one problem applied new problem deep learning type machine learning run input biologically inspired artificial neural network type learningncomputational learning theory ass learner computational complexity sample complexity much data required notion optimizationnnn natural language processing nnatural language processing nlp allows program read write communicate human language english specific problem include speech recognition speech synthesis machine translation information extraction information retrieval question answeringnearly work based noam chomsky generative grammar semantic network difficulty wordsense disambiguation unless restricted small domain called microworlds due common sense knowledge problem margaret masterman believed meaning grammar key understanding language thesaurus dictionary basis computational language structurenmodern deep learning technique nlp include word embedding representing word typically vector encoding meaning transformer deep learning architecture using attention mechanism others generative pretrained transformer gpt language model began generate coherent text model able get humanlevel score bar exam sat test gre test many realworld applicationsnnn perception nmachine perception ability use input sensor camera microphone wireless signal active lidar sonar radar tactile sensor deduce aspect world computer vision ability analyze visual inputnthe field includes speech recognition image classification facial recognition object recognition robotic perceptionnnn social intelligence nnaffective computing interdisciplinary umbrella comprises system recognize interpret process simulate human feeling emotion mood example virtual assistant programmed speak conversationally even banter humorously make appear sensitive emotional dynamic human interaction otherwise facilitate humancomputer interactionnhowever tends give nave user unrealistic conception intelligence existing computer agent moderate success related affective computing include textual sentiment analysis recently multimodal sentiment analysis wherein ai classifies affect displayed videotaped subjectnnn general intelligence na machine artificial general intelligence able solve wide variety problem breadth versatility similar human intelligencennn technique nai research us wide variety technique accomplish goal abovennn search optimization nai solve many problem intelligently searching many possible solution two different kind search used ai state space search local searchnnn state space search nstate space search search tree possible state try find goal state example planning algorithm search tree goal subgoals attempting find path target goal process called meansends analysisnsimple exhaustive search rarely sufficient realworld problem search space number place search quickly grows astronomical number result search slow never completes heuristic rule thumb help prioritize choice likely reach goalnadversarial search used gameplaying program chess go search tree possible move countermove looking winning positionnnn local search nlocal search us mathematical optimization find solution problem begin form guess refines incrementallyngradient descent type local search optimizes set numerical parameter incrementally adjusting minimize loss function variant gradient descent commonly used train neural networksnanother type local search evolutionary computation aim iteratively improve set candidate solution mutating recombining selecting fittest survive generationndistributed search process coordinate via swarm intelligence algorithm two popular swarm algorithm used search particle swarm optimization inspired bird flocking ant colony optimization inspired ant trailsnnn logic nformal logic used reasoning knowledge representationnformal logic come two main form propositional logic operates statement true false us logical connective implies predicate logic also operates object predicate relation us quantifier every x x ysndeductive reasoning logic process proving new statement conclusion statement given assumed true premise proof structured proof tree node labelled sentence child node connected parent node inference rulesngiven problem set premise problemsolving reduces searching proof tree whose root node labelled solution problem whose leaf node labelled premise axiom case horn clause problemsolving search performed reasoning forward premise backwards problem general case clausal form firstorder logic resolution single axiomfree rule inference problem solved proving contradiction premise include negation problem solvedninference horn clause logic firstorder logic undecidable therefore intractable however backward reasoning horn clause underpins computation logic programming language prolog turing complete moreover efficiency competitive computation symbolic programming languagesnfuzzy logic assigns degree truth therefore handle proposition vague partially truennonmonotonic logic including logic programming negation failure designed handle default reasoningnother specialized version logic developed describe many complex domainsnnn probabilistic method uncertain reasoning nnmany problem ai including reasoning planning learning perception robotics require agent operate incomplete uncertain information ai researcher devised number tool solve problem using method probability theory economics precise mathematical tool developed analyze agent make choice plan using decision theory decision analysis information value theory tool include model markov decision process dynamic decision network game theory mechanism designnbayesian network tool used reasoning using bayesian inference algorithm learning using expectationmaximization algorithm planning using decision network perception using dynamic bayesian networksnnprobabilistic algorithm also used filtering prediction smoothing finding explanation stream data thus helping perception system analyze process occur time eg hidden markov model kalman filtersnnn classifier statistical learning method nthe simplest ai application divided two type classifier eg shiny diamond one hand controller eg diamond pick hand classifier function use pattern matching determine closest match finetuned based chosen example using supervised learning pattern also called observation labeled certain predefined class observation combined class label known data set new observation received observation classified based previous experiencenthere many kind classifier use decision tree simplest widely used symbolic machine learning algorithm knearest neighbor algorithm widely used analogical ai mids kernel method support vector machine svm displaced knearest neighbor snthe naive bayes classifier reportedly widely used learner google due part scalabilitynneural network also used classifiersnnn artificial neural network nnan artificial neural network based collection node also known artificial neuron loosely model neuron biological brain trained recognise pattern trained recognise pattern fresh data input least one hidden layer node output node applies function weight cross specified threshold data transmitted next layer network typically called deep neural network least hidden layersnlearning algorithm neural network use local search choose weight get right output input training common training technique backpropagation algorithmnneural network learn model complex relationship input output find pattern data theory neural network learn functionnin feedforward neural network signal pass one direction recurrent neural network feed output signal back input allows shortterm memory previous input event long short term memory successful network architecture recurrent networksnperceptronsnuse single layer neuron deep learning us multiple layersnconvolutional neural network strengthen connection neuron close otherthis especially important image processing local set neuron must identify edge network identify objectnnn deep learning nndeep learningnuses several layer neuron network input output multiple layer progressively extract higherlevel feature raw input example image processing lower layer may identify edge higher layer may identify concept relevant human digit letter facesndeep learning profoundly improved performance program many important subfields artificial intelligence including computer vision speech recognition natural language processing image classification others reason deep learning performs well many application known nthe sudden success deep learning occur new discovery theoretical breakthrough deep neural network backpropagation described many people far back snbut two factor incredible increase computer power including hundredfold increase speed switching gpus availability vast amount training data especially giant curated datasets used benchmark testing imagenetnnn gpt ngenerative pretrained transformer gpt large language model based semantic relationship word sentence natural language processing textbased gpt model pretrained large corpus text internet pretraining consists predicting next token token usually word subword punctuation throughout pretraining gpt model accumulate knowledge world generate humanlike text repeatedly predicting next token typically subsequent training phase make model truthful useful harmless usually technique called reinforcement learning human feedback rlhf current gpt model still prone generating falsehood called hallucination although reduced rlhf quality data used chatbots allow ask question request task simple textncurrent model service include gemini formerly bard chatgpt grok claude copilot llama multimodal gpt model process different type data modality image video sound textnnn specialized hardware software nnin late graphic processing unit gpus increasingly designed aispecific enhancement used specialized tensorflow software replaced previously used central processing unit cpu dominant mean largescale commercial academic machine learning model training historically specialized language lisp prolog python others usednnn application nai machine learning technology used essential application including search engine google search targeting online advertisement recommendation system offered netflix youtube amazon driving internet traffic targeted advertising adsense facebook virtual assistant siri alexa autonomous vehicle including drone ada selfdriving car automatic language translation microsoft translator google translate facial recognition apple face id microsofts deepface google facenet image labeling used facebook apple iphoto tiktoknnn health medicine nnthe application ai medicine medical research potential increase patient care quality life lens hippocratic oath medical professional ethically compelled use ai application accurately diagnose treat patientsnfor medical research ai important tool processing integrating big data particularly important organoid tissue engineering development use microscopy imaging key technique fabrication suggested ai overcome discrepancy funding allocated different field research new ai tool deepen understanding biomedically relevant pathway example alphafold demonstrated ability approximate hour rather month structure protein reported aiguided drug discovery helped find class antibiotic capable killing two different type drugresistant bacteria researcher used machine learning accelerate search parkinson disease drug treatment aim identify compound block clumping aggregation alphasynuclein protein characterises parkinson disease able speed initial screening process tenfold reduce cost thousandfoldnnn game nngame playing program used since demonstrate test ai advanced technique deep blue became first computer chessplaying system beat reigning world chess champion garry kasparov may jeopardy quiz show exhibition match ibms question answering system watson defeated two greatest jeopardy champion brad rutter ken jennings significant margin march alphago game go match go champion lee sedol becoming first computer goplaying system beat professional go player without handicap defeated ke jie best go player world program handle imperfectinformation game pokerplaying program pluribus deepmind developed increasingly generalistic reinforcement learning model muzero could trained play chess go atari game deepminds alphastar achieved grandmaster level starcraft ii particularly challenging realtime strategy game involves incomplete knowledge happens map ai agent competed playstation gran turismo competition winning four world best gran turismo driver using deep reinforcement learningnnn military nnvarious country deploying ai military application main application enhance command control communication sensor integration interoperability research targeting intelligence collection analysis logistics cyber operation information operation semiautonomous autonomous vehicle ai technology enable coordination sensor effector threat detection identification marking enemy position target acquisition coordination deconfliction distributed joint fire networked combat vehicle involving manned unmanned team ai incorporated military operation iraq syrianin november u vice president kamala harris disclosed declaration signed nation set guardrail military use ai commitment include using legal review ensure compliance military ai international law cautious transparent development technologynnn generative ai nnin early generative ai gained widespread prominence march u adult heard chatgpt tried increasing realism easeofuse aibased texttoimage generator midjourney dalle stable diffusion sparked trend viral aigenerated photo widespread attention gained fake photo pope francis wearing white puffer coat fictional arrest donald trump hoax attack pentagon well usage professional creative artsnnn industryspecific task nthere also thousand successful ai application used solve specific problem specific industry institution survey one five company reported incorporated ai offering process example energy storage medical diagnosis military logistics application predict result judicial decision foreign policy supply chain managementnin agriculture ai helped farmer identify area need irrigation fertilization pesticide treatment increasing yield agronomist use ai conduct research development ai used predict ripening time crop tomato monitor soil moisture operate agricultural robot conduct predictive analytics classify livestock pig call emotion automate greenhouse detect disease pest save waternartificial intelligence used astronomy analyze increasing amount available data application mainly classification regression clustering forecasting generation discovery development new scientific insight example discovering exoplanets forecasting solar activity distinguishing signal instrumental effect gravitational wave astronomy could also used activity space space exploration including analysis data space mission realtime science decision spacecraft space debris avoidance autonomous operationnnn ethic nnai potential benefit potential risk ai may able advance science find solution serious problem demis hassabis deep mind hope solve intelligence use solve everything else however use ai become widespread several unintended consequence risk identified inproduction system sometimes factor ethic bias ai training process especially ai algorithm inherently unexplainable deep learningnnn risk harm nnn privacy copyright nnmachinelearning algorithm require large amount data technique used acquire data raised concern privacy surveillance copyrightntechnology company collect wide range data user including online activity geolocation data video audionfor example order build speech recognition algorithm amazon recorded million private conversation allowed temporary worker listen transcribe opinion widespread surveillance range see necessary evil clearly unethical violation right privacynai developer argue way deliver valuable application developed several technique attempt preserve privacy still obtaining data data aggregation deidentification differential privacy since privacy expert cynthia dwork begun view privacy term fairness brian christian wrote expert pivoted question know question theyre itngenerative ai often trained unlicensed copyrighted work including domain image computer code output used rationale fair use expert disagree well circumstance rationale hold court law relevant factor may include purpose character use copyrighted work effect upon potential market copyrighted work website owner wish content scraped indicate robotstxt file leading author including john grisham jonathan franzen sued ai company using work train generative ai another discussed approach envision separate sui generis system protection creation generated ai ensure fair attribution compensation human authorsnnn misinformation nnyoutube facebook others use recommender system guide user content ai program given goal maximizing user engagement goal keep people watching ai learned user tended choose misinformation conspiracy theory extreme partisan content keep watching ai recommended user also tended watch content subject ai led people filter bubble received multiple version misinformation convinced many user misinformation true ultimately undermined trust institution medium government ai program correctly learned maximize goal result harmful society u election major technology company took step mitigate problemnin generative ai began create image audio video text indistinguishable real photograph recording film human writing possible bad actor use technology create massive amount misinformation propaganda ai pioneer geoffrey hinton expressed concern ai enabling authoritarian leader manipulate electorate large scale among risksnnn algorithmic bias fairness nnmachine learning application biased learn biased data developer may aware bias existsnbias introduced way training data selected way model deployed biased algorithm used make decision seriously harm people medicine finance recruitment housing policing algorithm may cause discriminationnfairness machine learning study prevent harm caused algorithmic bias become serious area academic study within ai researcher discovered always possible define fairness way satisfies stakeholdersnon june google photo new image labeling feature mistakenly identified jacky alcine friend gorilla black system trained dataset contained image black people problem called sample size disparity google fixed problem preventing system labelling anything gorilla eight year later google photo still could identify gorilla neither could similar product apple facebook microsoft amazonncompas commercial program widely used u court ass likelihood defendant becoming recidivistnin julia angwin propublica discovered compas exhibited racial bias despite fact program told race defendant although error rate white black calibrated equal exactly error race differentthe system consistently overestimated chance black person would reoffend would underestimate chance white person would reoffend several researcher showed mathematically impossible compas accommodate possible measure fairness base rate reoffense different white black datana program make biased decision even data explicitly mention problematic feature race gender feature correlate feature like address shopping history first name program make decision based feature would race gendernmoritz hardt said robust fact research area fairness blindness doesnt workncriticism compas highlighted machine learning model designed make prediction valid assume future resemble past trained data includes result racist decision past machine learning model must predict racist decision made future application us prediction recommendation recommendation likely racist thus machine learning well suited help make decision area hope future better past necessarily descriptive proscriptivenbias unfairness may go undetected developer overwhelmingly white male among ai engineer black womennat conference fairness accountability transparency acm facct association computing machinery seoul south korea presented published finding recommend ai robotics system demonstrated free bias mistake unsafe use selflearning neural network trained vast unregulated source flawed internet data curtailednnn lack transparency nnmany ai system complex designer cannot explain reach decision particularly deep neural network large amount nonlinear relationship input output popular explainability technique existnit impossible certain program operating correctly one know exactly work many case machine learning program passed rigorous test nevertheless learned something different programmer intended example system could identify skin disease better medical professional found actually strong tendency classify image ruler cancerous picture malignancy typically include ruler show scale another machine learning system designed help effectively allocate medical resource found classify patient asthma low risk dying pneumonia asthma actually severe risk factor since patient asthma would usually get much medical care relatively unlikely die according training data correlation asthma low risk dying pneumonia real misleadingnpeople harmed algorithm decision right explanation doctor example expected clearly completely explain colleague reasoning behind decision make early draft european union general data protection regulation included explicit statement right exists industry expert noted unsolved problem solution sight regulator argued nevertheless harm real problem solution tool usedndarpa established xai explainable artificial intelligence program try solve problem nthere several possible solution transparency problem shap tried solve transparency problem visualising contribution feature output lime locally approximate model simpler interpretable model multitask learning provides large number output addition target classification output help developer deduce network learned deconvolution deepdream generative method allow developer see different layer deep network learned produce output suggest network learningnnn bad actor weaponized ai nnartificial intelligence provides number tool useful bad actor authoritarian government terrorist criminal rogue statesna lethal autonomous weapon machine locates selects engages human target without human supervision widely available ai tool used bad actor develop inexpensive autonomous weapon produced scale potentially weapon mass destruction even used conventional warfare unlikely unable reliably choose target could potentially kill innocent person nation including china supported ban autonomous weapon united nation convention certain conventional weapon however united state others disagreed fifty country reported researching battlefield robotsnai tool make easier authoritarian government efficiently control citizen several way face voice recognition allow widespread surveillance machine learning operating data classify potential enemy state prevent hiding recommendation system precisely target propaganda misinformation maximum effect deepfakes generative ai aid producing misinformation advanced ai make authoritarian centralized decision making competitive liberal decentralized system market lower cost difficulty digital warfare advanced spyware technology available since earlierai facial recognition system already used mass surveillance chinanthere many way ai expected help bad actor foreseen example machinelearning ai able design ten thousand toxic molecule matter hoursnnn reliance industry giant ntraining ai system requires enormous amount computing power usually big tech company financial resource make investment smaller startup cohere openai end buying access data center google microsoft respectivelynnn technological unemployment nneconomists frequently highlighted risk redundancy ai speculated unemployment adequate social policy full employmentnin past technology tended increase rather reduce total employment economist acknowledge uncharted territory ai survey economist showed disagreement whether increasing use robot ai cause substantial increase longterm unemployment generally agree could net benefit productivity gain redistributed risk estimate vary example michael osborne carl benedikt frey estimated u job high risk potential automation oecd report classified u job high risk methodology speculating future employment level criticised lacking evidential foundation implying technology rather social policy creates unemployment opposed redundancy april reported job chinese video game illustrator eliminated generative artificial intelligencenunlike previous wave automation many middleclass job may eliminated artificial intelligence economist stated worry ai could whitecollar job steam power bluecollar one industrial revolution worth taking seriously job extreme risk range paralegal fast food cook job demand likely increase carerelated profession ranging personal healthcare clergynfrom early day development artificial intelligence argument example put forward joseph weizenbaum whether task done computer actually done given difference computer human quantitative calculation qualitative valuebased judgementnnn existential risk nnit argued ai become powerful humanity may irreversibly lose control could physicist stephen hawking stated spell end human race scenario common science fiction computer robot suddenly develops humanlike selfawareness sentience consciousness becomes malevolent character scifi scenario misleading several waysnfirst ai require humanlike sentience existential risk modern ai program given specific goal use learning intelligence achieve philosopher nick bostrom argued one give almost goal sufficiently powerful ai may choose destroy humanity achieve used example paperclip factory manager stuart russell give example household robot try find way kill owner prevent unplugged reasoning cant fetch coffee youre dead order safe humanity superintelligence would genuinely aligned humanity morality value fundamentally sidensecond yuval noah harari argues ai require robot body physical control pose existential risk essential part civilization physical thing like ideology law government money economy made language exist story billion people believe current prevalence misinformation suggests ai could use language convince people believe anything even take action destructiventhe opinion amongst expert industry insider mixed sizable fraction concerned unconcerned risk eventual superintelligent ai personality stephen hawking bill gate elon musk expressed concern existential risk ainai pioneer including feifei li geoffrey hinton yoshua bengio cynthia breazeal rana el kaliouby demis hassabis joy buolamwini sam altman expressed concern risk ai many leading ai expert issued joint statement mitigating risk extinction ai global priority alongside societalscale risk pandemic nuclear warnother researcher however spoke favor le dystopian view ai pioneer juergen schmidhuber sign joint statement emphasising case ai research making human life longer healthier easier tool used improve life also used bad actor also used bad actor andrew ng also argued mistake fall doomsday hype aiand regulator benefit vested interest yann lecun scoff peer dystopian scenario supercharged misinformation even eventually human extinction early expert argued risk distant future warrant research human valuable perspective superintelligent machine however study current future risk possible solution became serious area researchnnn ethical machine alignment nnfriendly ai machine designed beginning minimize risk make choice benefit human eliezer yudkowsky coined term argues developing friendly ai higher research priority may require large investment must completed ai becomes existential risknmachines intelligence potential use intelligence make ethical decision field machine ethic provides machine ethical principle procedure resolving ethical dilemmasnthe field machine ethic also called computational moralitynand founded aaai symposium nother approach include wendell wallachs artificial moral agent stuart j russell three principle developing provably beneficial machinesnnn open source nactive organization ai opensource community include hugging face google eleutherai meta various ai model llama mistral stable diffusion made openweight meaning architecture trained parameter weight publicly available openweight model freely finetuned allows company specialize data usecase openweight model useful research innovation also misused since finetuned builtin security measure objecting harmful request trained away becomes ineffective researcher warn future ai model may develop dangerous capability potential drastically facilitate bioterrorism released internet cant deleted everywhere needed recommend prerelease audit costbenefit analysesnnn framework nartificial intelligence project ethical permissibility tested designing developing implementing ai system ai framework care act framework containing sum valuesdeveloped alan turing institute test project four main areasnnrespect dignity individual peoplenconnect people sincerely openly inclusivelyncare wellbeing everyonenprotect social value justice public interestnother development ethical framework include decided upon asilomar conference montreal declaration responsible ai ieees ethic autonomous system initiative among others however principle go without criticism especially regard people chosen contributes frameworksnpromotion wellbeing people community technology affect requires consideration social ethical implication stage ai system design development implementation collaboration job role data scientist product manager data engineer domain expert delivery managersnthe ai safety institute uk released testing toolset called inspect ai safety evaluation available mit opensource licence freely available github improved thirdparty package used evaluate ai model range area including core knowledge ability reason autonomous capabilitiesnnn regulation nnthe regulation artificial intelligence development public sector policy law promoting regulating artificial intelligence ai therefore related broader regulation algorithm regulatory policy landscape ai emerging issue jurisdiction globally according ai index stanford annual number airelated law passed survey country jumped one passed passed alone country adopted dedicated strategy ai eu member state released national ai strategy canada china india japan mauritius russian federation saudi arabia united arab emirate u vietnam others process elaborating ai strategy including bangladesh malaysia tunisia global partnership artificial intelligence launched june stating need ai developed accordance human right democratic value ensure public confidence trust technology henry kissinger eric schmidt daniel huttenlocher published joint statement november calling government commission regulate ai openai leader published recommendation governance superintelligence believe may happen le year united nation also launched advisory body provide recommendation ai governance body comprises technology company executive government official academicsnin ipsos survey attitude towards ai varied greatly country chinese citizen american agreed product service using ai benefit drawback reutersipsos poll found american agree disagree ai pose risk humanity fox news poll american thought important additional thought somewhat important federal government regulate ai versus responding important responding importantnin november first global ai safety summit held bletchley park uk discus near far term risk ai possibility mandatory voluntary regulatory framework country including united state china european union issued declaration start summit calling international cooperation manage challenge risk artificial intelligencennn history nnthe study mechanical formal reasoning began philosopher mathematician antiquity study logic led directly alan turing theory computation suggested machine shuffling symbol simple could simulate conceivable form mathematical reasoning along concurrent discovery cybernetics information theory neurobiology led researcher consider possibility building electronic brain nthey developed several area research would become part ainsuch mccullouch pitt design artificial neuron turing influential paper computing machinery intelligence introduced turing test showed machine intelligence plausiblenthe field ai research founded workshop dartmouth college attendee became leader ai research student produced program press described astonishing computer learning checker strategy solving word problem algebra proving logical theorem speaking english artificial intelligence laboratory set number british u university latter early snresearchers convinced method would eventually succeed creating machine general intelligence considered goal field herbert simon predicted machine capable within twenty year work man marvin minsky agreed writing within generation problem creating artificial intelligence substantially solved however underestimated difficulty problem u british government cut exploratory research response criticism sir james lighthill ongoing pressure u congress fund productive project minskys paperts book perceptrons understood proving artificial neural network would never useful solving realworld task thus discrediting approach altogether ai winter period obtaining funding ai project difficult followednin early ai research revived commercial success expert system form ai program simulated knowledge analytical skill human expert market ai reached billion dollar time japan fifth generation computer project inspired u british government restore funding academic research however beginning collapse lisp machine market ai fell disrepute second longerlasting winter begannup point ai funding gone project used highlevel symbol represent mental object like plan goal belief known fact researcher began doubt approach would able imitate process human cognition especially perception robotics learning pattern recognition began look subsymbolic approach rodney brook rejected representation general focussed directly engineering machine move survive judea pearl lofti zadeh others developed method handled incomplete uncertain information making reasonable guess rather precise logic important development revival connectionism including neural network research geoffrey hinton others yann lecun successfully showed convolutional neural network recognize handwritten digit first many successful application neural networksnai gradually restored reputation late early st century exploiting formal mathematical method finding specific solution specific problem narrow formal focus allowed researcher produce verifiable result collaborate field statistic economics mathematics solution developed ai researcher widely used although rarely described artificial intelligencenhowever several academic researcher became concerned ai longer pursuing original goal creating versatile fully intelligent machine beginning around founded subfield artificial general intelligence agi several wellfunded institution sndeep learning began dominate industry benchmark adopted throughout fieldnfor many specific task method abandonedndeep learning success based hardware improvement faster computer graphic processing unit cloud computing access large amount data including curated datasets imagenet deep learning success led enormous increase interest funding ai amount machine learning research measured total publication increased year nin issue fairness misuse technology catapulted center stage machine learning conference publication vastly increased funding became available many researcher refocussed career issue alignment problem became serious field academic studynin late teen early agi company began deliver program created enormous interest alphago developed deepmind beat world champion go player program taught rule game developed strategy gpt large language model released openai capable generating highquality humanlike text program others inspired aggressive ai boom large company began investing billion ai research according ai impact billion annually invested ai around u alone new u computer science phd graduate specialized ainabout airelated u job opening existed nnn philosophy nnn defining artificial intelligence nnalan turing wrote propose consider question machine think advised changing question whether machine think whether possible machinery show intelligent behaviour devised turing test measure ability machine simulate human conversation since observe behavior machine matter actually thinking literally mind turing note determine thing people usual polite convention everyone thinksnrussell norvig agree turing intelligence must defined term external behavior internal structure however critical test requires machine imitate human aeronautical engineering text wrote define goal field making machine fly exactly like pigeon fool pigeon ai founder john mccarthy agreed writing artificial intelligence definition simulation human intelligencenmccarthy defines intelligence computational part ability achieve goal world another ai founder marvin minsky similarly describes ability solve hard problem leading ai textbook defines study agent perceive environment take action maximize chance achieving defined goal definition view intelligence term welldefined problem welldefined solution difficulty problem performance program direct measure intelligence machineand philosophical discussion required may even possiblenanother definition adopted google major practitioner field ai definition stipulates ability system synthesize information manifestation intelligence similar way defined biological intelligencennn evaluating approach ai nno established unifying theory paradigm guided ai research history unprecedented success statistical machine learning eclipsed approach much source especially business world use term artificial intelligence mean machine learning neural network approach mostly subsymbolic soft narrow critic argue question may revisited future generation ai researchersnnn symbolic ai limit nsymbolic ai gofai simulated highlevel conscious reasoning people use solve puzzle express legal reasoning mathematics highly successful intelligent task algebra iq test newell simon proposed physical symbol system hypothesis physical symbol system necessary sufficient mean general intelligent actionnhowever symbolic approach failed many task human solve easily learning recognizing object commonsense reasoning moravecs paradox discovery highlevel intelligent task easy ai low level instinctive task extremely difficult philosopher hubert dreyfus argued since human expertise depends unconscious instinct rather conscious symbol manipulation feel situation rather explicit symbolic knowledge although argument ridiculed ignored first presented eventually ai research came agree himnthe issue resolved subsymbolic reasoning make many inscrutable mistake human intuition algorithmic bias critic noam chomsky argue continuing research symbolic ai still necessary attain general intelligence part subsymbolic ai move away explainable ai difficult impossible understand modern statistical ai program made particular decision emerging field neurosymbolic artificial intelligence attempt bridge two approachesnnn neat v scruffy nnneats hope intelligent behavior described using simple elegant principle logic optimization neural network scruffies expect necessarily requires solving large number unrelated problem neats defend program theoretical rigor scruffies rely mainly incremental testing see work issue actively discussed eventually seen irrelevant modern ai element bothnnn soft v hard computing nnfinding provably correct optimal solution intractable many important problem soft computing set technique including genetic algorithm fuzzy logic neural network tolerant imprecision uncertainty partial truth approximation soft computing introduced late successful ai program st century example soft computing neural networksnnn narrow v general ai nnai researcher divided whether pursue goal artificial general intelligence superintelligence directly solve many specific problem possible narrow ai hope solution lead indirectly field longterm goal general intelligence difficult define difficult measure modern ai verifiable success focusing specific problem specific solution experimental subfield artificial general intelligence study area exclusivelynnn machine consciousness sentience mind nnthe philosophy mind know whether machine mind consciousness mental state sense human being issue considers internal experience machine rather external behavior mainstream ai research considers issue irrelevant affect goal field build machine solve problem using intelligence russell norvig add additional project making machine conscious exactly way human one equipped take however question become central philosophy mind also typically central question issue artificial intelligence fictionnnn consciousness nndavid chalmers identified two problem understanding mind named hard easy problem consciousness easy problem understanding brain process signal make plan control behavior hard problem explaining feel feel like anything assuming right thinking truly feel like something dennetts consciousness illusionism say illusion human information processing easy explain human subjective experience difficult explain example easy imagine colorblind person learned identify object field view red clear would required person know red look likennn computationalism functionalism nncomputationalism position philosophy mind human mind information processing system thinking form computing computationalism argues relationship mind body similar identical relationship software hardware thus may solution mindbody problem philosophical position inspired work ai researcher cognitive scientist originally proposed philosopher jerry fodor hilary putnamnphilosopher john searle characterized position strong ai appropriately programmed computer right input output would thereby mind exactly sense human being mind searle counter assertion chinese room argument attempt show even machine perfectly simulates human behavior still reason suppose also mindnnn ai welfare right nit difficult impossible reliably evaluate whether advanced ai sentient ability feel degree significant chance given machine feel suffer may entitled certain right welfare protection measure similarly animal sapience set capacity related high intelligence discernment selfawareness may provide another moral basis ai right robot right also sometimes proposed practical way integrate autonomous agent societynin european union considered granting electronic personhood capable ai system similarly legal status company would conferred right also responsibility critic argued granting right ai system would downplay importance human right legislation focus user need rather speculative futuristic scenario also noted robot lacked autonomy take part society ownnprogress ai increased interest topic proponent ai welfare right often argue ai sentience emerges would particularly easy deny warn may moral blind spot analogous slavery factory farming could lead largescale suffering sentient ai created carelessly exploitednnn future nnn superintelligence singularity na superintelligence hypothetical agent would posse intelligence far surpassing brightest gifted human mindnif research artificial general intelligence produced sufficiently intelligent software might able reprogram improve improved software would even better improving leading j good called intelligence explosion vernor vinge called singularitynhowever technology cannot improve exponentially indefinitely typically follow sshaped curve slowing reach physical limit technology donnn transhumanism nrobot designer han moravec cyberneticist kevin warwick inventor ray kurzweil predicted human machine merge future cyborg capable powerful either idea called transhumanism root aldous huxley robert ettingernedward fredkin argues artificial intelligence next stage evolution idea first proposed samuel butler darwin among machine far back expanded upon george dyson book name nnn fiction nnthoughtcapable artificial being appeared storytelling device since antiquity persistent theme science fictionna common trope work began mary shelley frankenstein human creation becomes threat master includes work arthur c clarkes stanley kubrick space odyssey hal murderous computer charge discovery one spaceship well terminator matrix contrast rare loyal robot gort day earth stood still bishop alien le prominent popular culturenisaac asimov introduced three law robotics many book story notably multivac series superintelligent computer name asimov law often brought lay discussion machine ethic almost artificial intelligence researcher familiar asimov law popular culture generally consider law useless many reason one ambiguitynseveral work use ai force u confront fundamental question make u human showing u artificial being ability feel thus suffer appears karel apeks rur film ai artificial intelligence ex machina well novel android dream electric sheep philip k dick dick considers idea understanding human subjectivity altered technology created artificial intelligencennn see also nartificial intelligence detection software software detect aigenerated contentpages displaying short description redirect targetsnbehavior selection algorithm algorithm selects action intelligent agentsnbusiness process automation technologyenabled automation complex business processesncasebased reasoning process solving new problem based solution similar past problemsncomputational intelligence ability computer learn specific task data experimental observationndigital immortality hypothetical concept storing personality digital formnemergent algorithm algorithm exhibiting emergent behaviornfemale gendering ai technology gender bias digital technologypages displaying short description redirect targetsnglossary artificial intelligence list definition term concept commonly used study artificial intelligencenintelligence amplification use information technology augment human intelligencenmind uploading hypothetical process digitally emulating brainnrobotic process automation form business process automation technologynweak artificial intelligence form artificial intelligencenwetware computer computer composed organic materialnnn explanatory note nnn reference nnn ai textbook nthe two widely used textbook see open syllabusnnrussell stuart j norvig peter artificial intelligence modern approach th ed hoboken pearson isbn lccn nrich elaine knight kevin nair shivashankar b artificial intelligence rd ed new delhi tata mcgraw hill india isbn nthese four widely used ai textbook nnn history ai nnn source nnn reading nnn external link nnartificial intelligence internet encyclopedia philosophynthomason richmond logic artificial intelligence zalta edward n ed stanford encyclopedia philosophynartificial intelligence bbc radio discussion john agar alison adam igor aleksander time december ntheranostics ai next advance cancer precision medicine',\n 'neural network group interconnected unit called neuron send signal one another neuron either biological cell mathematical model individual neuron simple many together network perform complex task two main type neural networknnin neuroscience biological neural network physical structure found brain complex nervous system population nerve cell connected synapsesnin machine learning artificial neural network mathematical model used approximate nonlinear function artificial neural network used solve artificial intelligence problemsnnn biology nnin context biology neural network population biological neuron chemically connected synapsis given neuron connected hundred thousand synapsesneach neuron sends receives electrochemical signal called action potential connected neighbor neuron serve excitatory role amplifying propagating signal receives inhibitory role suppressing signal insteadnpopulations interconnected neuron smaller neural network called neural circuit large interconnected network called large scale brain network many together form brain nervous systemsnsignals generated neural network brain eventually travel nervous system across neuromuscular junction muscle cell cause contraction thereby motionnnn machine learning nnin context machine learning neural network artificial mathematical model used approximate nonlinear function early artificial neural network physical machine today almost always implemented softwarenneurons artificial neural network usually arranged layer information passing first layer input layer one intermediate layer hidden layer final layer output layernthe signal input neuron number specifically linear combination output connected neuron previous layer signal neuron output calculated number according activation function behavior network depends strength weight connection neuron network trained modifying weight empirical risk minimization backpropagation order fit preexisting datasetnneural network used solve problem artificial intelligence thereby found application many discipline including predictive modeling adaptive control facial recognition handwriting recognition general game playing generative ainnn history nnthe theoretical base contemporary neural network independently proposed alexander bain william james posited human thought emerged interaction among large number neuron inside brain donald hebb described hebbian learning idea neural network change learn time strengthening synapse every time signal travel along itnartificial neural network originally used model biological neural network starting approach connectionism however starting invention perceptron simple artificial neural network warren mcculloch walter pitt followed implementation one hardware frank rosenblatt nartificial neural network became increasingly used machine learning application instead increasingly different biological counterpartsnnn see also nemergencenbiological cyberneticsnbiologicallyinspired computingnnn reference',\n 'deep learning subset machine learning method based neural network representation learning adjective deep refers use multiple layer network method used either supervised semisupervised unsupervisedndeeplearning architecture deep neural network deep belief network recurrent neural network convolutional neural network transformer applied field including computer vision speech recognition natural language processing machine translation bioinformatics drug design medical image analysis climate science material inspection board game program produced result comparable case surpassing human expert performancenearly form neural network inspired information processing distributed communication node biological system particular human brain however current neural network intend model brain function organism generally seen low quality model purposennn overview nmost modern deep learning model based multilayered neural network convolutional neural network transformer although also include propositional formula latent variable organized layerwise deep generative model node deep belief network deep boltzmann machinesnfundamentally deep learning refers class machine learning algorithm hierarchy layer used transform input data slightly abstract composite representation example image recognition model raw input may image represented tensor pixel first representational layer may attempt identify basic shape line circle second layer may compose encode arrangement edge third layer may encode nose eye fourth layer may recognize image contains facenimportantly deep learning process learn feature optimally place level prior deep learning machine learning technique often involved handcrafted feature engineering transform data suitable representation classification algorithm operate upon deep learning approach feature handcrafted model discovers useful feature representation data automatically eliminate need handtuning example varying number layer layer size provide different degree abstractionnthe word deep deep learning refers number layer data transformed precisely deep learning system substantial credit assignment path cap depth cap chain transformation input output cap describe potentially causal connection input output feedforward neural network depth cap network number hidden layer plus one output layer also parameterized recurrent neural network signal may propagate layer cap depth potentially unlimited universally agreedupon threshold depth divide shallow learning deep learning researcher agree deep learning involves cap depth higher cap depth shown universal approximator sense emulate function beyond layer add function approximator ability network deep model cap able extract better feature shallow model hence extra layer help learning feature effectivelyndeep learning architecture constructed greedy layerbylayer method deep learning help disentangle abstraction pick feature improve performancendeep learning algorithm applied unsupervised learning task important benefit unlabeled data abundant labeled data example deep structure trained unsupervised manner deep belief networksnnn interpretation ndeep neural network generally interpreted term universal approximation theorem probabilistic inferencenthe classic universal approximation theorem concern capacity feedforward neural network single hidden layer finite size approximate continuous function first proof published george cybenko sigmoid activation function generalised feedforward multilayer architecture kurt hornik recent work also showed universal approximation also hold nonbounded activation function kunihiko fukushimas rectified linear unitnthe universal approximation theorem deep neural network concern capacity network bounded width depth allowed grow lu et al proved width deep neural network relu activation strictly larger input dimension network approximate lebesgue integrable function width smaller equal input dimension deep neural network universal approximatornthe probabilistic interpretation derives field machine learning feature inference well optimization concept training testing related fitting generalization respectively specifically probabilistic interpretation considers activation nonlinearity cumulative distribution function probabilistic interpretation led introduction dropout regularizer neural network probabilistic interpretation introduced researcher including hopfield widrow narendra popularized survey one bishopnnn history nthere two type artificial neural network ann feedforward neural network fnns recurrent neural network rnns rnns cycle connectivity structure fnns dont wilhelm lenz ernst ising created analyzed ising model essentially nonlearning rnn architecture consisting neuronlike threshold element shunichi amari made architecture adaptive learning rnn popularised john hopfield ncharles tappert writes frank rosenblatt developed explored basic ingredient deep learning system today referring rosenblatts book introduced multilayer perceptron mlp layer input layer hidden layer randomized weight learn output layer also introduced variant including version fourlayer perceptrons last two layer learned weight thus proper multilayer perceptronuasection ua addition term deep learning proposed rina dechter although history appearance apparently complicatednthe first general working learning algorithm supervised deep feedforward multilayer perceptrons published alexey ivakhnenko lapa paper described deep network eight layer trained group method data handlingnthe first deep learning multilayer perceptron trained stochastic gradient descent published shunichi amari computer experiment conducted amaris student saito five layer mlp two modifiable layer learned internal representation classify nonlinearily separable pattern class matthew brand reported wide layer nonlinear perceptrons could fully endtoend trained reproduce logic function nontrivial circuit depth via gradient descent small batch random inputoutput sample concluded training time contemporary hardware submegaflop computer made technique impractical proposed using fixed random early layer input hash single modifiable layer instead subsequent development hardware hyperparameter tuning made endtoend stochastic gradient descent currently dominant training techniquenin seppo linnainmaa published reverse mode automatic differentiation discrete connected network nested differentiable function became known backpropagation efficient application chain rule derived gottfried wilhelm leibniz network differentiable node nthe terminology backpropagating error actually introduced rosenblatt know implement although henry j kelley continuous precursor backpropagation already context control theory paul werbos applied backpropagation mlps way become standard david e rumelhart et al published experimental analysis techniquendeep learning architecture convolutional neural network cnns convolutional layer downsampling layer began neocognitron introduced kunihiko fukushima also introduced relu rectified linear unit activation function rectifier become popular activation function cnns deep learning general cnns become essential tool computer visionnthe term deep learning introduced machine learning community rina dechter artificial neural network igor aizenberg colleague context boolean threshold neuronsnin wei zhang et al applied backpropagation algorithm nto convolutional neural network simplified neocognitron convolutional interconnection image feature layer last fully connected layer alphabet recognition also proposed implementation cnn optical computing system nin yann lecun et al applied backpropagation cnn purpose recognizing handwritten zip code mail algorithm worked training required day subsequently wei zhang et al modified model removing last fully connected layer applied medical image object segmentation breast cancer detection mammogram lenet level cnn yann lecun et al classifies digit applied several bank recognize handwritten number check digitized x pixel imagesnin backpropagation work well deep learning long credit assignment path overcome problem jrgen schmidhuber proposed hierarchy rnns pretrained one level time selfsupervised learning us predictive coding learn internal representation multiple selforganizing time scale substantially facilitate downstream deep learning rnn hierarchy collapsed single rnn distilling higher level chunker network lower level automatizer network chunker solved deep learning task whose depth exceeded nin jrgen schmidhuber also published alternative rnns called linear transformer transformer linearized selfattention save normalization operator learns internal spotlight attention slow feedforward neural network learns gradient descent control fast weight another neural network outer product selfgenerated activation pattern called key value selfattention fast weight attention mapping applied query patternnthe modern transformer introduced ashish vaswani et al paper attention need nit combine softmax operator projection matrixntransformers increasingly become model choice natural language processing many modern large language model chatgpt gpt bert use transformer also increasingly used computer visionnin jrgen schmidhuber also published adversarial neural network contest form zerosum game one network gain network loss first network generative model model probability distribution output pattern second network learns gradient descent predict reaction environment pattern called artificial curiosity principle used generative adversarial network gan ian goodfellow et al environmental reaction depending whether first network output given set used create realistic deepfakes excellent image quality achieved nvidias stylegan based progressive gan tero karras et al gan generator grown small large scale pyramidal fashionnsepp hochreiters diploma thesis called one important document history machine learning supervisor schmidhuber tested neural history compressor also identified analyzed vanishing gradient problem hochreiter proposed recurrent residual connection solve problem led deep learning method called long shortterm memory lstm published lstm recurrent neural network learn deep learning task long credit assignment path require memory event happened thousand discrete time step vanilla lstm forget gate introduced felix gers schmidhuber fred cummins lstm become cited neural network th centurynin rupesh kumar srivastava klaus greff schmidhuber used lstm principle create highway network feedforward neural network hundred layer much deeper previous network month later kaiming xiangyu zhang shaoqing ren jian sun imagenet competition opengated gateless highway network variant called residual neural network become cited neural network st centurynin andr de carvalho together mike fairhurst david bisset published experimental result multilayer boolean neural network also known weightless neural network composed layer selforganising feature extraction neural network module soft followed multilayer classification neural network module gsn independently trained layer feature extraction module extracted feature growing complexity regarding previous layernin brendan frey demonstrated possible train two day network containing six fully connected layer several hundred hidden unit using wakesleep algorithm codeveloped peter dayan hintonnsince sven behnke extended feedforward hierarchical convolutional approach neural abstraction pyramid lateral backward connection order flexibly incorporate context decision iteratively resolve local ambiguitiesnsimpler model use taskspecific handcrafted feature gabor filter support vector machine svms popular choice artificial neural network computational cost lack understanding brain wire biological networksnboth shallow deep learning eg recurrent net anns speech recognition explored many year method never outperformed nonuniform internalhandcrafting gaussian mixture modelhidden markov model gmmhmm technology based generative model speech trained discriminatively key difficulty analyzed including gradient diminishing weak temporal correlation structure neural predictive model additional difficulty lack training data limited computing power speech recognition researcher moved away neural net pursue generative modeling exception sri international late funded u government nsa darpa sri studied deep neural network dnns speech speaker recognition speaker recognition team led larry heck reported significant success deep neural network speech processing national institute standard technology speaker recognition evaluation sri deep neural network deployed nuance verifier representing first major industrial application deep learning principle elevating raw feature handcrafted optimization first explored successfully architecture deep autoencoder raw spectrogram linear filterbank feature late showing superiority melcepstral feature contain stage fixed transformation spectrogram raw feature speech waveform later produced excellent largerscale resultsnspeech recognition taken lstm lstm started become competitive traditional speech recognizers certain task alex graf santiago fernndez faustino gomez schmidhuber combined connectionist temporal classification ctc stack lstm rnns google speech recognition reportedly experienced dramatic performance jump ctctrained lstm made available google voice searchnthe impact deep learning industry began early cnns already processed estimated check written u according yann lecun industrial application deep learning largescale speech recognition started around nin publication geoff hinton ruslan salakhutdinov osindero teh showed manylayered feedforward neural network could effectively pretrained one layer time treating layer turn unsupervised restricted boltzmann machine finetuning using supervised backpropagation paper referred learning deep belief netsnthe nip workshop deep learning speech recognition motivated limitation deep generative model speech possibility given capable hardware largescale data set deep neural net might become practical believed pretraining dnns using generative model deep belief net dbn would overcome main difficulty neural net however discovered replacing pretraining large amount training data straightforward backpropagation using dnns large contextdependent output layer produced error rate dramatically lower thenstateoftheart gaussian mixture model gmmhidden markov model hmm also moreadvanced generative modelbased system nature recognition error produced two type system characteristically different offering technical insight integrate deep learning existing highly efficient runtime speech decoding system deployed major speech recognition system analysis around contrasting gmm generative speech model v dnn model stimulated early industrial investment deep learning speech recognition analysis done comparable performance le error rate discriminative dnns generative modelsnin researcher extended deep learning timit large vocabulary speech recognition adopting large output layer dnn based contextdependent hmm state constructed decision treesndeep learning part stateoftheart system various discipline particularly computer vision automatic speech recognition asr result commonly used evaluation set timit asr mnist image classification well range largevocabulary speech recognition task steadily improved convolutional neural network superseded asr ctc lstm successful computer visionnadvances hardware driven renewed interest deep learning nvidia involved called big bang deep learning deeplearning neural network trained nvidia graphic processing unit gpus year andrew ng determined gpus could increase speed deeplearning system time particular gpus wellsuited matrixvector computation involved machine learning gpus speed training algorithm order magnitude reducing running time week day specialized hardware algorithm optimization used efficient processing deep learning modelsnnn deep learning revolution nnin late deep learning started outperform method machine learning competitionsnin long shortterm memory trained connectionist temporal classification alex graf santiago fernndez faustino gomez jrgen schmidhuber first rnn win pattern recognition contest winning three competition connected handwriting recognition google later used ctctrained lstm speech recognition smartphonensignificant impact image object recognition felt although cnns trained backpropagation around decade gpu implementation nns year including cnns faster implementation cnns gpus needed progress computer vision dannet dan ciresan ueli meier jonathan masci luca maria gambardella jrgen schmidhuber achieved first time superhuman performance visual pattern recognition contest outperforming traditional method factor also dannet icdar chinese handwriting contest may isbi image segmentation contest cnns play major role computer vision conference june paper ciresan et al leading conference cvpr showed maxpooling cnns gpu dramatically improve many vision benchmark record september dannet also icpr contest analysis large medical image cancer detection following year also miccai grand challenge topic october similar alexnet alex krizhevsky ilya sutskever geoffrey hinton largescale imagenet competition significant margin shallow machine learning method nthe vgg network karen simonyan andrew zisserman reduced error rate andnwon imagenet competition following similar trend largescale speech recognitionnimage classification extended challenging task generating description caption image often combination cnns lstmsnin team led george e dahl merck molecular activity challenge using multitask deep neural network predict biomolecular target one drug sepp hochreiters group used deep learning detect offtarget toxic effect environmental chemical nutrient household product drug tox data challenge nih fda ncatsnin roger parloff mentioned deep learning revolution transformed ai industrynin march yoshua bengio geoffrey hinton yann lecun awarded turing award conceptual engineering breakthrough made deep neural network critical component computingnnn neural network nnartificial neural network anns connectionist system computing system inspired biological neural network constitute animal brain system learn progressively improve ability task considering example generally without taskspecific programming example image recognition might learn identify image contain cat analyzing example image manually labeled cat cat using analytic result identify cat image found use application difficult express traditional computer algorithm using rulebased programmingnan ann based collection connected unit called artificial neuron analogous biological neuron biological brain connection synapse neuron transmit signal another neuron receiving postsynaptic neuron process signal signal downstream neuron connected neuron may state generally represented real number typically neuron synapsis may also weight varies learning proceeds increase decrease strength signal sends downstreamntypically neuron organized layer different layer may perform different kind transformation input signal travel first input last output layer possibly traversing layer multiple timesnthe original goal neural network approach solve problem way human brain would time attention focused matching specific mental ability leading deviation biology backpropagation passing information reverse direction adjusting network reflect informationnneural network used variety task including computer vision speech recognition machine translation social network filtering playing board video game medical diagnosisnas neural network typically thousand million unit million connection despite number several order magnitude le number neuron human brain network perform many task level beyond human eg recognizing face playing gonnn deep neural network na deep neural network dnn artificial neural network multiple layer input output layer different type neural network always consist component neuron synapsis weight bias function component whole function way mimic function human brain trained like ml algorithmnfor example dnn trained recognize dog breed go given image calculate probability dog image certain breed user review result select probability network display certain threshold etc return proposed label mathematical manipulation considered layer complex dnn many layer hence name deep networksndnns model complex nonlinear relationship dnn architecture generate compositional model object expressed layered composition primitive extra layer enable composition feature lower layer potentially modeling complex data fewer unit similarly performing shallow network instance proved sparse multivariate polynomial exponentially easier approximate dnns shallow networksndeep architecture include many variant basic approach architecture found success specific domain always possible compare performance multiple architecture unless evaluated data setsndnns typically feedforward network data flow input layer output layer without looping back first dnn creates map virtual neuron assigns random numerical value weight connection weight input multiplied return output network accurately recognize particular pattern algorithm would adjust weight way algorithm make certain parameter influential determines correct mathematical manipulation fully process datanrecurrent neural network data flow direction used application language modeling long shortterm memory particularly effective usenconvolutional neural network cnns used computer vision cnns also applied acoustic modeling automatic speech recognition asrnnn challenge na anns many issue arise naively trained dnns two common issue overfitting computation timendnns prone overfitting added layer abstraction allow model rare dependency training data regularization method ivakhnenkos unit pruning weight decay n n n n n n n n n n n n displaystyle ell n nregularization sparsity n n n n n n n n n n n n displaystyle ell n nregularization applied training combat overfitting alternatively dropout regularization randomly omits unit hidden layer training help exclude rare dependency finally data augmented via method cropping rotating smaller training set increased size reduce chance overfittingndnns must consider many training parameter size number layer number unit per layer learning rate initial weight sweeping parameter space optimal parameter may feasible due cost time computational resource various trick batching computing gradient several training example rather individual example speed computation large processing capability manycore architecture gpus intel xeon phi produced significant speedup training suitability processing architecture matrix vector computationsnalternatively engineer may look type neural network straightforward convergent training algorithm cmac cerebellar model articulation controller one kind neural network doesnt require learning rate randomized initial weight training process guaranteed converge one step new batch data computational complexity training algorithm linear respect number neuron involvednnn hardware nsince advance machine learning algorithm computer hardware led efficient method training deep neural network contain many layer nonlinear hidden unit large output layer graphic processing unit gpus often aispecific enhancement displaced cpu dominant method training largescale commercial cloud ai openai estimated hardware computation used largest deep learning project alexnet alphazero found fold increase amount computation required doublingtime trendline monthsnspecial electronic circuit called deep learning processor designed speed deep learning algorithm deep learning processor include neural processing unit npus huawei cellphone cloud computing server tensor processing unit tpu google cloud platform cerebras system also built dedicated system handle large deep learning model c based largest processor industry secondgeneration wafer scale engine wsenatomically thin semiconductor considered promising energyefficient deep learning hardware basic device structure used logic operation data storagenin marega et al published experiment largearea active channel material developing logicinmemory device circuit based floatinggate fieldeffect transistor fgfetsnin j feldmann et al proposed integrated photonic hardware accelerator parallel convolutional processing author identify two key advantage integrated photonics electronic counterpart massively parallel data transfer wavelength division multiplexing conjunction frequency comb extremely high data modulation speed system execute trillion multiplyaccumulate operation per second indicating potential integrated photonics dataheavy ai applicationsnnn application nnn automatic speech recognition nnlargescale automatic speech recognition first convincing successful case deep learning lstm rnns learn deep learning task involve multisecond interval containing speech event separated thousand discrete time step one time step corresponds m lstm forget gate competitive traditional speech recognizers certain tasksnthe initial success speech recognition based smallscale recognition task based timit data set contains speaker eight major dialect american english speaker read sentence small size let many configuration tried importantly timit task concern phonesequence recognition unlike wordsequence recognition allows weak phone bigram language model let strength acoustic modeling aspect speech recognition easily analyzed error rate listed including early result measured percent phone error rate per summarized since nnthe debut dnns speaker recognition late speech recognition around lstm around accelerated progress eight major areasnnscaleupout accelerated dnn training decodingnsequence discriminative trainingnfeature processing deep model solid understanding underlying mechanismsnadaptation dnns related deep modelsnmultitask transfer learning dnns related deep modelsncnns design best exploit domain knowledge speechnrnn rich lstm variantsnother type deep model including tensorbased model integrated deep generativediscriminative modelsnall major commercial speech recognition system eg microsoft cortana xbox skype translator amazon alexa google apple siri baidu iflytek voice search range nuance speech product etc based deep learningnnn image recognition nna common evaluation set image classification mnist database data set mnist composed handwritten digit includes training example test example timit small size let user test multiple configuration comprehensive list result set availablendeep learningbased image recognition become superhuman producing accurate result human contestant first occurred recognition traffic sign recognition human facesndeep learningtrained vehicle interpret camera view another example facial dysmorphology novel analysis fdna used analyze case human malformation connected large database genetic syndromesnnn visual art processing nnclosely related progress made image recognition increasing application deep learning technique various visual art task dnns proven capable example ofnnidentifying style period given paintingnneural style transfer capturing style given artwork applying visually pleasing manner arbitrary photograph videongenerating striking imagery based random visual input fieldsnnn natural language processing nnneural network used implementing language model since early lstm helped improve machine translation language modelingnother key technique field negative sampling word embedding word embedding wordvec thought representational layer deep learning architecture transforms atomic word positional representation word relative word dataset position represented point vector space using word embedding rnn input layer allows network parse sentence phrase using effective compositional vector grammar compositional vector grammar thought probabilistic context free grammar pcfg implemented rnn recursive autoencoders built atop word embeddings ass sentence similarity detect paraphrasing deep neural architecture provide best result constituency parsing sentiment analysis information retrieval spoken language understanding machine translation contextual entity linking writing style recognition namedentity recognition token classification text classification othersnrecent development generalize word embedding sentence embeddingngoogle translate gt us large endtoend long shortterm memory lstm network google neural machine translation gnmt us examplebased machine translation method system learns million example translates whole sentence time rather piece google translate support one hundred language network encodes semantics sentence rather simply memorizing phrasetophrase translation gt us english intermediate language pairsnnn drug discovery toxicology nna large percentage candidate drug fail win regulatory approval failure caused insufficient efficacy ontarget effect undesired interaction offtarget effect unanticipated toxic effect research explored use deep learning predict biomolecular target offtargets toxic effect environmental chemical nutrient household product drugsnatomnet deep learning system structurebased rational drug design atomnet used predict novel candidate biomolecules disease target ebola virus multiple sclerosisnin graph neural network used first time predict various property molecule large toxicology data set generative neural network used produce molecule validated experimentally way micennn customer relationship management nndeep reinforcement learning used approximate value possible direct marketing action defined term rfm variable estimated value function shown natural interpretation customer lifetime valuennn recommendation system nnrecommendation system used deep learning extract meaningful feature latent factor model contentbased music journal recommendation multiview deep learning applied learning user preference multiple domain model us hybrid collaborative contentbased approach enhances recommendation multiple tasksnnn bioinformatics nnan autoencoder ann used bioinformatics predict gene ontology annotation genefunction relationshipsnin medical informatics deep learning used predict sleep quality based data wearable prediction health complication electronic health record datandeep neural network shown unparalleled performance predicting protein structure according sequence amino acid make alphafold deeplearning based system achieved level accuracy significantly higher previous computational methodsnnn deep neural network estimation ndeep neural network used estimate entropy stochastic process called neural joint entropy estimator njee estimation provides insight effect input random variable independent random variable practically dnn trained classifier map input vector matrix x output probability distribution possible class random variable given input x example image classification task njee map vector pixel color value probability possible image class practice probability distribution obtained softmax layer number node equal alphabet size njee us continuously differentiable activation function condition universal approximation theorem hold shown method provides strongly consistent estimator outperforms method case large alphabet sizesnnn medical image analysis ndeep learning shown produce competitive result medical application cancer cell classification lesion detection organ segmentation image enhancement modern deep learning tool demonstrate high accuracy detecting various disease helpfulness use specialist improve diagnosis efficiencynnn mobile advertising nfinding appropriate mobile audience mobile advertising always challenging since many data point must considered analyzed target segment created used ad serving ad server deep learning used interpret large manydimensioned advertising datasets many data point collected requestserveclick internet advertising cycle information form basis machine learning improve ad selectionnnn image restoration ndeep learning successfully applied inverse problem denoising superresolution inpainting film colorization application include learning method shrinkage field effective image restoration train image dataset deep image prior train image need restorationnnn financial fraud detection ndeep learning successfully applied financial fraud detection tax evasion detection antimoney launderingnnn material science nin november researcher google deepmind lawrence berkeley national laboratory announced developed ai system known gnome system contributed material science discovering million new material within relatively short timeframe gnome employ deep learning technique efficiently explore potential material structure achieving significant increase identification stable inorganic crystal structure system prediction validated autonomous robotic experiment demonstrating noteworthy success rate data newly discovered material publicly available material project database offering researcher opportunity identify material desired property various application development implication future scientific discovery integration ai material science research potentially expediting material innovation reducing cost product development use ai deep learning suggests possibility minimizing eliminating manual lab experiment allowing scientist focus design analysis unique compoundsnnn military nthe united state department defense applied deep learning train robot new task observationnnn partial differential equation nphysics informed neural network used solve partial differential equation forward inverse problem data driven manner one example reconstructing fluid flow governed navierstokes equation using physic informed neural network require often expensive mesh generation conventional cfd method relies onnnn image reconstruction nimage reconstruction reconstruction underlying image imagerelated measurement several work showed better superior performance deep learning method compared analytical method various application eg spectral imaging ultrasound imagingnnn epigenetic clock nnan epigenetic clock biochemical test used measure age galkin et al used deep neural network train epigenetic aging clock unprecedented accuracy using blood sample clock us information cpg site predicts people certain condition older healthy control ibd frontotemporal dementia ovarian cancer obesity aging clock planned released public use insilico medicine spinoff company deep longevitynnn relation human cognitive brain development ndeep learning closely related class theory brain development specifically neocortical development proposed cognitive neuroscientist early developmental theory instantiated computational model making predecessor deep learning system developmental model share property various proposed learning dynamic brain eg wave nerve growth factor support selforganization somewhat analogous neural network utilized deep learning model like neocortex neural network employ hierarchy layered filter layer considers information prior layer operating environment pass output possibly original input layer process yield selforganizing stack transducer welltuned operating environment description stated infant brain seems organize influence wave socalled trophicfactors different region brain become connected sequentially one layer tissue maturing another whole brain maturena variety approach used investigate plausibility deep learning model neurobiological perspective one hand several variant backpropagation algorithm proposed order increase processing realism researcher argued unsupervised form deep learning based hierarchical generative model deep belief network may closer biological reality respect generative neural network model related neurobiological evidence samplingbased processing cerebral cortexnalthough systematic comparison human brain organization neuronal encoding deep network yet established several analogy reported example computation performed deep learning unit could similar actual neuron neural population similarly representation developed deep learning model similar measured primate visual system singleunit population levelsnnn commercial activity nfacebooks ai lab performs task automatically tagging uploaded picture name people themngoogles deepmind technology developed system capable learning play atari video game using pixel data input demonstrated alphago system learned game go well enough beat professional go player google translate us neural network translate languagesnin covariantai launched focus integrating deep learning factoriesnas researcher university texas austin ut developed machine learning framework called training agent manually via evaluative reinforcement tamer proposed new method robot computer program learn perform task interacting human instructor first developed tamer new algorithm called deep tamer later introduced collaboration u army research laboratory arl ut researcher deep tamer used deep learning provide robot ability learn new task observation using deep tamer robot learned task human trainer watching video stream observing human perform task inperson robot later practiced task help coaching trainer provided feedback good job bad jobnnn criticism comment ndeep learning attracted criticism comment case outside field computer sciencennn theory nna main criticism concern lack theory surrounding method learning common deep architecture implemented using wellunderstood gradient descent however theory surrounding algorithm contrastive divergence le clear eg converge fast approximating deep learning method often looked black box confirmation done empirically rather theoreticallynothers point deep learning looked step towards realizing strong ai allencompassing solution despite power deep learning method still lack much functionality needed realize goal entirely research psychologist gary marcus notednnrealistically deep learning part larger challenge building intelligent machine technique lack way representing causal relationship obvious way performing logical inference also still long way integrating abstract knowledge information object typically used powerful ai system like watson use technique like deep learning one element complicated ensemble technique ranging statistical technique bayesian inference deductive reasoningnnin reference idea artistic sensitivity might inherent relatively low level cognitive hierarchy published series graphic representation internal state deep layer neural network attempting discern within essentially random data image trained demonstrate visual appeal original research notice received well comment subject time frequently accessed article guardian websitennn error nsome deep learning architecture display problematic behavior confidently classifying unrecognizable image belonging familiar category ordinary image misclassifying minuscule perturbation correctly classified image goertzel hypothesized behavior due limitation internal representation limitation would inhibit integration heterogeneous multicomponent artificial general intelligence agi architecture issue may possibly addressed deep learning architecture internally form state homologous imagegrammar decomposition observed entity event learning grammar visual linguistic training data would equivalent restricting system commonsense reasoning operates concept term grammatical production rule basic goal human language acquisition artificial intelligence ainnn cyber threat na deep learning move lab world research experience show artificial neural network vulnerable hack deception identifying pattern system use function attacker modify input anns way ann find match human observer would recognize example attacker make subtle change image ann find match even though image look human nothing like search target manipulation termed adversarial attacknin researcher used one ann doctor image trial error fashion identify anothers focal point thereby generate image deceived modified image looked different human eye another group showed printout doctored image photographed successfully tricked image classification system one defense reverse image search possible fake image submitted site tineye find instance refinement search using part image identify image piece may takennanother group showed certain psychedelic spectacle could fool facial recognition system thinking ordinary people celebrity potentially allowing one person impersonate another researcher added sticker stop sign caused ann misclassify themnanns however trained detect attempt deception potentially leading attacker defender arm race similar kind already defines malware defense industry anns trained defeat annbased antimalware software repeatedly attacking defense malware continually altered genetic algorithm tricked antimalware retaining ability damage targetnin another group demonstrated certain sound could make google voice command system open particular web address hypothesized could serve stepping stone attack eg opening web page hosting driveby malwarenin data poisoning false data continually smuggled machine learning system training set prevent achieving masterynnn data collection ethic nnmost deep learning system rely training verification data generated andor annotated human argued medium philosophy lowpaid clickwork eg amazon mechanical turk regularly deployed purpose also implicit form human microwork often recognized philosopher rainer mhlhoff distinguishes five type machinic capture human microwork generate training data gamification embedding annotation computation task flow game trapping tracking eg captchas image recognition clicktracking google search result page exploitation social motivation eg tagging face facebook obtain labeled facial image information mining eg leveraging quantifiedself device activity tracker clickworknmhlhoff argues commercial enduser application deep learning facebooks face recognition system need training data stop ann trained rather continued demand humangenerated verification data constantly calibrate update ann purpose facebook introduced feature user automatically recognized image receive notification choose whether like publicly labeled image tell facebook picture user interface mechanism generate constant stream verification data train network realtime mhlhoff argues involvement human user generate training verification data typical commercial enduser application deep learning system may referred humanaided artificial intelligencennn see also napplications artificial intelligencencomparison deep learning softwarencompressed sensingndifferentiable programmingnecho state networknlist artificial intelligence projectsnliquid state machinenlist datasets machinelearning researchnreservoir computingnscale space deep learningnsparse codingnstochastic parrotntopological deep learningnnn reference nnn reading']"},"metadata":{}}]},{"cell_type":"code","source":"chars = sorted(list(set(\"\".join(processed_docs))))\nchar_to_index = {char: idx for idx, char in enumerate(chars)}\nindex_to_char = {idx: char for idx, char in enumerate(chars)}\nnum_chars = len(chars)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:29.605680Z","iopub.execute_input":"2024-05-17T22:09:29.605969Z","iopub.status.idle":"2024-05-17T22:09:29.646349Z","shell.execute_reply.started":"2024-05-17T22:09:29.605946Z","shell.execute_reply":"2024-05-17T22:09:29.645521Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"max_sequence_len = 1000\n\ndef prepare_sequences(text):\n    sequences = []\n    next_chars = []\n    for i in range(0, len(text) - max_sequence_len, 1):\n        seq = text[i:i + max_sequence_len]\n        next_char = text[i + max_sequence_len]\n        sequences.append([char_to_index[char] for char in seq])\n        next_chars.append(char_to_index[next_char])\n    return sequences, next_chars\n\n# Prepare sequences for each document\nchars_sequences = []\nnext_chars_list = []\nfor doc in processed_docs:\n    sequences, next_chars = prepare_sequences(doc)\n    chars_sequences.extend(sequences)\n    next_chars_list.extend(next_chars)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:29.647464Z","iopub.execute_input":"2024-05-17T22:09:29.647806Z","iopub.status.idle":"2024-05-17T22:09:38.152229Z","shell.execute_reply.started":"2024-05-17T22:09:29.647776Z","shell.execute_reply":"2024-05-17T22:09:38.151216Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sequences_padded = pad_sequences(chars_sequences, maxlen=max_sequence_len, padding='pre')\nX = np.array(sequences_padded)\ny = to_categorical(next_chars_list, num_classes=num_chars)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:38.153439Z","iopub.execute_input":"2024-05-17T22:09:38.153757Z","iopub.status.idle":"2024-05-17T22:09:45.219028Z","shell.execute_reply.started":"2024-05-17T22:09:38.153733Z","shell.execute_reply":"2024-05-17T22:09:45.218246Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:45.219955Z","iopub.execute_input":"2024-05-17T22:09:45.220205Z","iopub.status.idle":"2024-05-17T22:09:45.365439Z","shell.execute_reply.started":"2024-05-17T22:09:45.220183Z","shell.execute_reply":"2024-05-17T22:09:45.364520Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(57879, 1000)\n(38587, 1000)\n(57879, 27)\n(38587, 27)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the RNN model\nchar_model1 = Sequential([\n    Embedding(input_dim=num_chars, output_dim=100, input_length=max_sequence_len),\n    SimpleRNN(100),\n    Dense(num_chars, activation='softmax')\n])\nchar_model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nchar_model1.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:45.366614Z","iopub.execute_input":"2024-05-17T22:09:45.366901Z","iopub.status.idle":"2024-05-17T22:09:46.298992Z","shell.execute_reply.started":"2024-05-17T22:09:45.366877Z","shell.execute_reply":"2024-05-17T22:09:46.297699Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"char_model1.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:09:46.300457Z","iopub.execute_input":"2024-05-17T22:09:46.300772Z","iopub.status.idle":"2024-05-17T22:35:24.705382Z","shell.execute_reply.started":"2024-05-17T22:09:46.300748Z","shell.execute_reply":"2024-05-17T22:35:24.704530Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m   2/1809\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 84ms/step - accuracy: 0.0391 - loss: 3.3039 ","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715983790.394696     128 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 86ms/step - accuracy: 0.1126 - loss: 3.0184 - val_accuracy: 0.1852 - val_loss: 2.7083\nEpoch 2/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 84ms/step - accuracy: 0.2091 - loss: 2.6181 - val_accuracy: 0.2744 - val_loss: 2.4252\nEpoch 3/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.2868 - loss: 2.3588 - val_accuracy: 0.3276 - val_loss: 2.2353\nEpoch 4/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.3462 - loss: 2.1682 - val_accuracy: 0.3957 - val_loss: 2.0261\nEpoch 5/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.4163 - loss: 1.9523 - val_accuracy: 0.4425 - val_loss: 1.8838\nEpoch 6/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.4600 - loss: 1.8115 - val_accuracy: 0.4773 - val_loss: 1.7800\nEpoch 7/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.4954 - loss: 1.7067 - val_accuracy: 0.4960 - val_loss: 1.7190\nEpoch 8/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.5145 - loss: 1.6289 - val_accuracy: 0.5056 - val_loss: 1.6894\nEpoch 9/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.5332 - loss: 1.5705 - val_accuracy: 0.5179 - val_loss: 1.6429\nEpoch 10/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 85ms/step - accuracy: 0.5476 - loss: 1.5278 - val_accuracy: 0.5269 - val_loss: 1.6214\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b10e07f4e80>"},"metadata":{}}]},{"cell_type":"code","source":"loss, accuracy = char_model1.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", accuracy*100)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:35:24.706566Z","iopub.execute_input":"2024-05-17T22:35:24.706857Z","iopub.status.idle":"2024-05-17T22:35:48.199398Z","shell.execute_reply.started":"2024-05-17T22:35:24.706832Z","shell.execute_reply":"2024-05-17T22:35:48.198536Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\u001b[1m1206/1206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - accuracy: 0.5297 - loss: 1.6132\nTest Accuracy: 52.688729763031006\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the RNN model\nchar_model2 = Sequential([\n    Embedding(input_dim=num_chars, output_dim=300, input_length=max_sequence_len),\n    SimpleRNN(128),\n    Dense(num_chars, activation='softmax')\n])\nchar_model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nchar_model2.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:35:48.200690Z","iopub.execute_input":"2024-05-17T22:35:48.201313Z","iopub.status.idle":"2024-05-17T22:35:48.229328Z","shell.execute_reply.started":"2024-05-17T22:35:48.201278Z","shell.execute_reply":"2024-05-17T22:35:48.228497Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ simple_rnn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"char_model2.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T22:35:48.230277Z","iopub.execute_input":"2024-05-17T22:35:48.230543Z","iopub.status.idle":"2024-05-17T23:11:59.838298Z","shell.execute_reply.started":"2024-05-17T22:35:48.230520Z","shell.execute_reply":"2024-05-17T23:11:59.837293Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m   1/1809\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31:53\u001b[0m 3s/step - accuracy: 0.0312 - loss: 3.3317","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1715985351.499658     129 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.2603 - loss: 2.4952","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1715985529.087322     128 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 122ms/step - accuracy: 0.2603 - loss: 2.4951 - val_accuracy: 0.4008 - val_loss: 2.0315\nEpoch 2/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 120ms/step - accuracy: 0.4215 - loss: 1.9463 - val_accuracy: 0.4649 - val_loss: 1.8131\nEpoch 3/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 120ms/step - accuracy: 0.4871 - loss: 1.7253 - val_accuracy: 0.5013 - val_loss: 1.7046\nEpoch 4/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 120ms/step - accuracy: 0.5182 - loss: 1.6148 - val_accuracy: 0.5213 - val_loss: 1.6435\nEpoch 5/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 120ms/step - accuracy: 0.5417 - loss: 1.5360 - val_accuracy: 0.5340 - val_loss: 1.5935\nEpoch 6/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 120ms/step - accuracy: 0.5547 - loss: 1.4840 - val_accuracy: 0.5348 - val_loss: 1.5834\nEpoch 7/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 120ms/step - accuracy: 0.5705 - loss: 1.4413 - val_accuracy: 0.5484 - val_loss: 1.5480\nEpoch 8/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 120ms/step - accuracy: 0.5724 - loss: 1.4124 - val_accuracy: 0.5498 - val_loss: 1.5409\nEpoch 9/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 119ms/step - accuracy: 0.5842 - loss: 1.3742 - val_accuracy: 0.5511 - val_loss: 1.5283\nEpoch 10/10\n\u001b[1m1809/1809\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 119ms/step - accuracy: 0.5857 - loss: 1.3695 - val_accuracy: 0.5586 - val_loss: 1.5131\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7b10546b1390>"},"metadata":{}}]},{"cell_type":"code","source":"loss, accuracy = char_model2.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", accuracy*100)","metadata":{"execution":{"iopub.status.busy":"2024-05-17T23:11:59.839748Z","iopub.execute_input":"2024-05-17T23:11:59.840067Z","iopub.status.idle":"2024-05-17T23:12:43.279065Z","shell.execute_reply.started":"2024-05-17T23:11:59.840041Z","shell.execute_reply":"2024-05-17T23:12:43.278207Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u001b[1m1206/1206\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 36ms/step - accuracy: 0.5587 - loss: 1.5107\nTest Accuracy: 55.86337447166443\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_random_start_char(processed_docs):\n    all_text = ''.join(processed_docs)  \n    return random.choice(all_text)  # Randomly select one character","metadata":{"execution":{"iopub.status.busy":"2024-05-17T23:22:40.977891Z","iopub.execute_input":"2024-05-17T23:22:40.978223Z","iopub.status.idle":"2024-05-17T23:22:40.982718Z","shell.execute_reply.started":"2024-05-17T23:22:40.978198Z","shell.execute_reply":"2024-05-17T23:22:40.981805Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def predict_next_chars_single(model, start_char, num_prediction_steps, char_to_index, index_to_char, max_sequence_len):\n    \"\"\"\n    Predict the next characters starting from a single character.\n\n    Parameters:\n    model -- Trained RNN model.\n    start_char -- The initial character to start the prediction from.\n    num_prediction_steps -- Number of characters to predict.\n    char_to_index -- Dictionary mapping characters to their index.\n    index_to_char -- Dictionary mapping index to their characters.\n    max_sequence_len -- The maximum length of the sequence used for prediction.\n\n    Returns:\n    predicted_text -- The initial character followed by the predicted characters.\n    \"\"\"\n    # Initialize the sequence with the start character\n    current_sequence = start_char\n    predicted_text = start_char\n\n    for _ in range(num_prediction_steps):\n        # Convert the current sequence to indices\n        sequence_indices = [char_to_index[char] for char in current_sequence[-max_sequence_len:]]\n        \n        # Pad the sequence to the required length\n        padded_sequence_indices = pad_sequences([sequence_indices], maxlen=max_sequence_len, padding='pre')\n        \n        # Predict the next character index\n        predicted_index = np.argmax(model.predict(padded_sequence_indices), axis=-1)[0]\n        \n        # Convert the index back to a character\n        predicted_char = index_to_char[predicted_index]\n        \n        # Append the predicted character to the predicted text\n        predicted_text += predicted_char\n        \n        # Update the current sequence with the new character\n        current_sequence += predicted_char\n\n    return predicted_text","metadata":{"execution":{"iopub.status.busy":"2024-05-17T23:22:39.064304Z","iopub.execute_input":"2024-05-17T23:22:39.064965Z","iopub.status.idle":"2024-05-17T23:22:39.072287Z","shell.execute_reply.started":"2024-05-17T23:22:39.064935Z","shell.execute_reply":"2024-05-17T23:22:39.071334Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"start_char = get_random_start_char(processed_docs)\nprint(f\"Randomly selected starting character: '{start_char}'\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T23:25:44.472734Z","iopub.execute_input":"2024-05-17T23:25:44.473083Z","iopub.status.idle":"2024-05-17T23:25:44.478153Z","shell.execute_reply.started":"2024-05-17T23:25:44.473056Z","shell.execute_reply":"2024-05-17T23:25:44.477098Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Randomly selected starting character: 'a'\n","output_type":"stream"}]},{"cell_type":"code","source":"# Predict the next 10 characters starting from the randomly selected character\npredicted_text = predict_next_chars_single(char_model2, start_char, 10, char_to_index, index_to_char, max_sequence_len)\n\nprint(f\"Starting with '{start_char}', predicted text: {predicted_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-17T23:25:46.250277Z","iopub.execute_input":"2024-05-17T23:25:46.250965Z","iopub.status.idle":"2024-05-17T23:25:47.023285Z","shell.execute_reply.started":"2024-05-17T23:25:46.250936Z","shell.execute_reply":"2024-05-17T23:25:47.022482Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nStarting with 'a', predicted text: ai research\n","output_type":"stream"}]}]}